[{"id":0,"href":"/notes/CSCB09/Week-1/","title":"Week 1","section":"CSCB09","content":" Week 1 # Processes # What happens when you run a program.\nHas Input and Output ends:\nstdin: Standard Input - eg. Keyboard input goes through stdin stdout: Standard Output - eg. Terminal output (printing to screen) eg.\nNavinns-MacBook-Pro:~ home$ sort # OS connects keyboard to stdin Watermelon Apple Strawberry Mango ^D # \u0026#39;exit\u0026#39; keybind, OS connects screen to stdout Apple Mango Strawberry Watermelon Pipelining and Redirection # Routing outputs of one program to the input of another, and outputting to a file instead of the screen.\neg.\nNavinns-MacBook-Pro:Desktop home$ cat myfile # screen is connected to stdout # starting text This this is is a file haha haha Navinns-MacBook-Pro:Desktop home$ cat myfile | sort | uniq \u0026gt; myNewFile \u0026amp;\u0026amp; cat myNewFile # myfile -\u0026gt; stdin of cat =\u0026gt; stdout -\u0026gt; stdin of sort =\u0026gt; stdout -\u0026gt; stdin of uniq =\u0026gt; stdout -\u0026gt; myNewFile This a file haha is this Scripting # Combining shell commands, instead of being redundant.\neg.\nRedundant commands:\nNavinns-MacBook-Pro:Desktop home$ cat mywords | sort | uniq \u0026gt; mywords-unique Navinns-MacBook-Pro:Desktop home$ cat yourwords | sort | uniq \u0026gt; yourwords -unique Navinns-MacBook-Pro:Desktop home$ cat badwords | sort | uniq \u0026gt; badwords -unique Simple For-Loop script:\nfor w in mywords yourwords badwords; do cat $w | sort | uniq \u0026gt; $w-unique done Devices and Services # Presented as files in Unix.\nUnix kernal creates file names and emulates file operations, known as \u0026lsquo;Special Files\u0026rsquo; eg.\n/dev/sda # Hard disk as a special file, restricted /dev/zero # Endless stream of 0\u0026#39;s as bytes # To wipe a hard disk clean: Navinns-MacBook-Pro:~ home$ dd bs=4K if=/dev/zero of=/dev/sda Terminology # Kernel: Decides which processes may run, and what it can access\nOS Processes: More services, features, and background monitoring not in the kernal\nShell: Command-line user interface\nUser Processes: Your processes\nThe Unix Philosophy # Consisting of small programs, that do one thing really well\nScripting + Pipelining versus rewriting bigger programs for complex tasks\nRecursive file-like structure\nShort and Simple program names - eg. cp: copies files\n"},{"id":1,"href":"/notes/CSCA48/","title":"CSCA48","section":"Home","content":" CSCA48 - Intro to Computer Science II # Semester Taken: Winter 2020\nProgramming Language: C\nCourse Description:\nAbstract data types and data structures for implementing them Linked data structures Object Oriented Programming Encapsulation and information-hiding Testing Specifications Analyzing the efficiency of programs Recursion Course Page\n"},{"id":2,"href":"/notes/CSCC24/Week-1/","title":"Week 1","section":"CSCC24","content":" Week 1 # Translation # The process of converting high-level source code into machine code.\nA Programming Language (PL) is neither compiled or interpreted, it\u0026rsquo;s implementation can be.\nTypes of Translation # Compilation: translated before execution\n[Source Code] -\u0026gt; {Compiler} -\u0026gt; (Target Code) Target code optimises for speed and security verification (potential memory leaks) for the specific machine\n\u0026lsquo;Heavy\u0026rsquo; information (i.e. variable types) only needed during compilation Can expose potential errors/bugs before run-time\nC, Rust are complied languages\nInterpretation: translated line by line\n[Source Code] -\u0026gt; (Interpreter) Useful for prototyping, scripting\nEasier to debug; stack trace uses the source code rather than binaries\nMore portable; only requires an interpreter\nPython, JavaScript, Shell are interpreted languages\nPseudo-Compilation: hybrid of compilation and interpretation\n[Source Code] -\u0026gt; {Compiler} -\u0026gt; [(Intermediate Code)] -\u0026gt; (Interpreter) Compiler translate all of the source code before execution but into Intermediate Code\nInterpreter executes the Intermediate Code line by line\nIntermediate Code can be executed on any machine with the interpreter\nJava is a hybrid language, it\u0026rsquo;s intermediate code is bytecode\nThe Process of Translation # Lexical Analysis:\nCreates a sequence of tokens; the smallest meaningful element (language specific) Syntactic Analysis\nStructures tokens into an Initial Parse Tree\ninternal nodes are syntax rules, leaves are tokens Rules are determined by a Precedence Table Where syntax errors are discovered\nSymantic Analysis\nAnnotates parse tree with semantic actions Code Generation\nProduces machine code "},{"id":3,"href":"/notes/CSCB09/","title":"CSCB09","section":"Home","content":" CSCB09 - Software Tools and Systems Programming # Semester Taken: Summer 2020\nProgramming Languages: Bash shell, C\nCourse Description:\nSoftware techniques in a Unix-style environment, using scripting languages and a machine-oriented programming language (typically C). What goes on in the system when programs are executed.\nCore topics:\nCreating and using software tools, pipes and filters File processing, shell programming, processes, system calls, signals, basic network programming. Course Page\n"},{"id":4,"href":"/notes/CSCB09/Week-2/","title":"Week 2","section":"CSCB09","content":" Week 2 # File Management # How we survived without File Explorer\nDirectory Tree # / ├── Applications ├── Library ├── System ├── *Users* ├── Volumes ├── bin ├── cores ├── *dev* ├── etc -\u0026gt; private/etc ├── *home* -\u0026gt; /System/Volumes/Data/home ├── opt ├── private ├── sbin ├── tmp -\u0026gt; private/tmp ├── *usr* └── var -\u0026gt; private/var Paths # Absolute Path: from root dir\neg. /Users/home/projects/course-notes/index.html Relative Path: from current dir\neg. course-notes/index.html (if current dir is /Users/home/projects/) Parent Directory: ..\nDirectory Itself: .\nSome commands require the dir name, . simplifies this task Commands # pwd: print working directory\ncd: change directory\nls: list\n-a || -A: include dotfiles (. for -a, .. \u0026amp;\u0026amp; . for -A) -l: include more information (size, modification time, etc.) -d: include only directories -t: sort by modification time -r: reverse order -R: recursive list, whole subtree mkdir: make directory\ncp: copy\n-R: recursive copy, can possibly overwrite mv: move\nCan possibly replace existing files rmdir: remove dir\nPrecondition: the current dir is empty rm: remove/delete file(s)\n-r: recursive delete, used for non-empty directories No \u0026ldquo;Recycle Bin\u0026rdquo;/Trash Core Utilities # cat: concatenation\nCopy files and/or stdin to stdout eg. $ cat file1 file2 ... # dumps file1 ... # dumps file2 $ cat file1 - file2 ... # dumps file1 ... # user input as stdin, use ctrl+D to exit and continue ... # dumps file2 $ sort myFile | uniq | cat file1 - file2 ... # dumps file1 ... # dumps myFile after sorting and uniq ... # dumps file2 head and tail:\nhead starts from start of file, tail starts from last eg. $ head -3 # output first 3 lines $ head -n -3 # output all except last 3 lines $ tail -3 # output last 3 lines $ tail -n +3 # output starting from and including line 3 wc: word count\nCan count by words (-w), bytes (-c), lines (-l), characters (-m) eg. $ wc file1 2 6 27 file1 # 2 lines, 6 words, 27 bytes sort:\nSort, check if sorted, merge sorted Default sort by whole-line eg. # sample input Navinn CS 420 UofT Expensive 69 GiveJobPls iNeedJob 180 $ sort -b -k 3,3n # sort by key=3rd field, parse 3rd field as number UofT Expensive 69 # lowest number GiveJobPls iNeedJob 180 Navinn CS 420 # greatest number tr: translate\nSimilar to a String.replace() method\neg.\n$ tr 12 ab # replace \u0026#39;1\u0026#39; with \u0026#39;a\u0026#39;, \u0026#39;2\u0026#39; with \u0026#39;b\u0026#39; $ tr 1234 \u0026#39;[a*]\u0026#39; # replace \u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;,\u0026#39;4\u0026#39; with \u0026#39;a\u0026#39; $ tr A-Z a-z # replace Uppercase with lowercase -c: compliment\neg. $ tr -c 1234 \u0026#39;[a*]\u0026#39; # replace everything except \u0026#39;1\u0026#39;,\u0026#39;2\u0026#39;,\u0026#39;3\u0026#39;,\u0026#39;4\u0026#39; with \u0026#39;a\u0026#39; -s: squeeze\nReplace consecutive occurences by a single occurrence eg. $ tr -s 12 # can convert \u0026#39;11122abc211\u0026#39; to \u0026#39;12abc21\u0026#39; $ tr -s 12 abc # replaces then squeezes $ tr -cs 0-9a-z ’[\\n*]’ # replace everything except digits and letters with line breaks # then squeezes multiple line breaks -\u0026gt; \u0026#39;one word per line\u0026#39; -d: delete\nRemove occurence eg. $ tr -d 12 # removes \u0026#39;1\u0026#39; and 2\u0026#39; $ tr -ds 12 3 # deletes \u0026#39;1\u0026#39;,\u0026#39;2\u0026#39; then squeezes \u0026#39;3\u0026#39; tee: T\nBranching out, extra pipe (T-shaped) Copy stdin to stdout, and also copy to file (extra branch) eg. sort | tee sortedfile | uniq # similar to sort | uniq # but stdout from sort is now copied to sorted file before pipelining into uniq stdin yes:\nUnending lines of y Use case: if a program requires [y/N] a lot [expletive]: outputs expletive instead of y "},{"id":5,"href":"/notes/CSCC24/Week-2/","title":"Week 2","section":"CSCC24","content":" Week 2 # Programming Languages define syntax formally.\nLexical Rules: specify the form of the \u0026lsquo;building blocks\u0026rsquo; of a PL\ni.e. Comment Syntax, Tokens (keywords, literals, operators, etc.) and delimiters, White space Syntax: specifies how the \u0026lsquo;building blocks\u0026rsquo; are put together\nRegular Expressions # Notation # Kleene Star (*): 0 or more repetitions\nAlternation (+ or |): denotes choice\nGrouping: marked by parentheses ()\nEmpty/Null String: denoted by epsilon: $\\epsilon$\nEmpty Language: language with no strings, denoted by $\\empty$\nExamples:\n(0+1)*\t0 or more of (\u0026#34;0\u0026#34; OR \u0026#34;1\u0026#34;) 1*(:+;)*\t0+ of \u0026#34;1\u0026#34;, 0+ of (\u0026#34;:\u0026#34; OR \u0026#34;;\u0026#34;) (a+b)*aa(a+b)* 0+ of (\u0026#34;a\u0026#34; OR \u0026#34;b\u0026#34;), \u0026#34;aa\u0026#34;, 0+ of (\u0026#34;a\u0026#34; OR \u0026#34;b\u0026#34;) Formal Definition # Let $\\Sigma$ denote a finite alphabet. A given string is a Regular Expression (RE) iff it can be formed from primitive REs $$ [\\empty, \\epsilon, \\textrm{ any } a | a \\in \\Sigma ] $$\nRules of Applications of Regular Expressions # $a$ and $b$ are regular expressions\n$a + b$: Union\n$ab$ : Concatenation\n$a^\\star$: Kleene Closure\n$(a)$: Grouping\n$\\Sigma^\\star$: the set of all finite strings over the alphabet $\\Sigma$\nDefinition of a Regular Language # Languages that can be expressed by Regular Expressions.\nGiven a RE $r$ over an alphabet $\\Sigma$, $L(r)$, a language associated with $r$ is defined by:\n$L(\\empty)$: language that contains no strings\n$L(\\epsilon)$: language that only contains ${\\epsilon}$\nFor $a \\in \\Sigma$, $L(a)$: language that only contains ${a}$\nRules of Regular Languages # $a$ and $b$ are regular expressions\n$L(a + b) = L(a) \\cup L(b)$\n$L(ab) = L(a)L(b)$\n$L((a)) = L(a)$\n$L(a^\\star) = (L(a))^\\star$\n$S^\\star$ is the smallest superset of $S$ containing $\\epsilon$ and is closed under concatenation Grammars # Informal Grammar: just rules to follow i.e. \u0026ldquo;Don\u0026rsquo;t end a sentence with a preposition\u0026rdquo; Formal Grammar Language: set of strings\nGrammar: specifies which strings are in the language\nTypes of Formal Grammar # Chomsky\u0026rsquo;s Hierarchy: ordered by ascending expressiveness.\nRegular Grammars Context-Free Grammars Context-Sensitive Grammars Phase-Structure Grammars Context-Free Grammars # More powerful than REs.\nParts of a Context Free Grammar # Terminals: atomic symbols of the language\nNon-Terminals: \u0026ldquo;variables\u0026rdquo; used in the grammar\nStart Symbol: special non-terminal chosen as the top-level construct of the language\nThe first non-terminal listed is chosen as the start symbol by convention Productions: set of rules that specify legal ways that a non-terminal can be rewritten as a sequence of terminals and non-terminals\nFormal Definition of CFGs # A Context-Free Grammar $G$ is a tuple ${\\Sigma, V, S, P}$ where\n$\\Sigma$: set of terminals\n$V$: set of non-terminals\n$S$: Start Symbol where $S \\in V$\n$P$: set of productions\nRule Form: $X \\Longrightarrow w$, where $X \\in V, w \\in (\\Sigma \\cup V)^\\star$ Note: $\\Sigma \\cap V = \\empty$\nA Production Rule $X \\Longrightarrow w$ can be applied to a string in the form of $sXt$ where $s,t \\in (\\Sigma \\cup V)^\\star$ to produce a string $swt$\nA grammar $G$ generates string $s$ if $\\exists$ a finite sequence of applications of productions, such that the first rule is applied to the start symbol, and the last rule produces $s$, $s \\in \\Sigma^\\star$. Written as $G \\Rightarrow^\\star s$.\nDefinition of a Context-Free Language # Languages that can be expressed by Context-Free Grammars.\nThe language generated by a grammar $G$ is $L(G) = {s \\mid G \\Rightarrow^\\star s}$\nRegular Grammars # More restricted than CFGs.\nProductions in a Regular Grammar # Left Recursive: 0 or 1 non-terminal, appears as the left-most symbol on the RHS of the rule\n\u0026lt;S\u0026gt; ::= \u0026lt;T\u0026gt; a b\t\u0026lt;T\u0026gt; ::= a | \u0026lt;T\u0026gt; b Right Recursive: 0 or 1 non-terminal, appears as the right-most symbol on the RHS of rule\n\u0026lt;S\u0026gt; ::= a \u0026lt;T\u0026gt; \u0026lt;T\u0026gt; ::= b \u0026lt;T\u0026gt; | a \u0026lt;T\u0026gt; Backus-Naur Form (BNF) # The notation typically used to describe Context-Free Grammars.\nNon-Terminals: \u0026lt;NonTerminal\u0026gt; OR: | Productions: LHS ::= RHS or LHS ==\u0026gt; RHS Extended Backus-Naur Form (EBNF) # Extensions to BNF to make it more concise, but not more powerful\nRepetitions: { x } - 0 or more repetitions of x\n$x^+$: one or more repetitions $x^n$: maximum of $n$ repetitions Optional: [ x ] - 0 or 1 occurrences of x\nGrouping: ()\nDerivations # The sequence of applications of productions to generate a string.\nA string is in the language generated by a grammar iff there is a derivation for it Example, Write 3.14 using the following CFG:\n\u0026lt;r\u0026gt; ==\u0026gt; \u0026lt;p\u0026gt; . \u0026lt;p\u0026gt; \u0026lt;p\u0026gt; ==\u0026gt; \u0026lt;d\u0026gt; | \u0026lt;d\u0026gt; \u0026lt;p\u0026gt; \u0026lt;d\u0026gt; ==\u0026gt; 0 | ... | 9\t# ... just means 1 through 8 Solution:\n\u0026lt;r\u0026gt; ==\u0026gt; \u0026lt;p\u0026gt; . \u0026lt;p\u0026gt; ==\u0026gt; \u0026lt;d\u0026gt; . \u0026lt;d\u0026gt; \u0026lt;p\u0026gt; ==\u0026gt; 3 . 1 \u0026lt;d\u0026gt; ==\u0026gt; 3 . 1 4 Parse Trees # Shows the structure within a string of the language.\nParsing is the process of producing a parse tree.\nA string is in the language generated by a grammar iff there is a parse tree for the string.\nStructure of a Parse Tree # Root: start symbol\nLeaf: terminal\nInternal Node: non-terminal\nChildren correspond in-order, to the RHS of one of its products in the grammar Syntactic Ambiguity # A grammar is ambiguous iff it generates a string for where there are two or more Parse Trees\nA string is ambiguous w/r/t a grammar iff that grammar generates two or more Parse Trees\nNote: Two or more derivations does not mean a string is ambiguous.\nDealing with Ambiguity # Include Delimiters\ni.e. Curly Brackets, fi, etc. Impose Associativity and Precedence\ni.e. Multiplication gets higher precedence than Addition\nIntroduce a non-terminal for each precedence level\nFor the same precedence level, operators can be\nLeft Associative: recursive term before non-recursive\nRight Associative: recursive term after non-recursive\n"},{"id":6,"href":"/notes/CSCB09/Week-3/","title":"Week 3","section":"CSCB09","content":" Week 3 # Introduction to Shell # More features available from man sh\nSimple Commands # 4 general cases of commands\nBuilt in commands: cd, ls etc. User defined functions Aliases eg. alias pls=\u0026quot;sudo\u0026quot; Commands that refer to the program name eg. tr command runs the tr program Sequential List # Multi-line commands in a single line\n$ cd projects ; sort file1 | uniq # runs cd then runs sort with pipe # equivalent to $ cd projects projects$ sort file1 | uniq Grouping: Explicit: {grep foo file1 ; ls ; } \u0026gt; file2 Exit Codes # 0 for success, non-0 for failure\n$ sort file1 # assume this works $ echo $? # prints `?` command 0 $ cd asdgkj -bash: cd: asdgkj: No such file or directory $ echo $? 127 # non-zero -\u0026gt; fail Logical AND OR NOT # $ mkdir foo \u0026amp;\u0026amp; cd foo # sequential unless one fails $ mkdir foo1 || mkdir foo2 || exit # runs sequential command if first one fails $ ! mkdir foo # trivial (true to false, v.v) Operator Precedence:\n| highest ! \u0026amp;\u0026amp; || - same precedence ; lowest Boolean Conditions # String comparisons: [ s1 = s2 ] # `!=`, `\u0026lt;`, `\u0026gt;` also work Number comparisons: [ num1 -eq num2 ] # `-ne`,`-gt`,`-ge`,`-lt`,`-le` also work Logical connectives -a: and -o: or -e: not Conditionals and Iteratives # if statement: if list1 ; then list2 elif list 3 ; then list4 else list5 fi # end statement while loop: while list1 ; do list2 done # end statement for loop: for var in word1 word2 word3 ; do echo $var mkdir $var done # end statement Filename Patters # *: any string ?: matches one character [nav]: matches \u0026rsquo;n\u0026rsquo;, \u0026lsquo;a\u0026rsquo;, \u0026lsquo;v\u0026rsquo; [0-9]: matches digit [!0-9]: matches non-digit eg.\nfor i in *.sh ; do # for all files in dir with ext .sh echo $i # echo its name done Escaping and Quoting # \\: backslash '': single quotes \u0026quot;\u0026quot;: double quotes Use Cases:\nSpaced files: cd \u0026quot;Onedrive - University of Toronto\u0026quot; Regex for grep: grep ’\u0026lt;[a-z]*\u0026gt;’ *.html Operators in test commands: [ ! ’(’ aaa ’\u0026lt;’ abc -o aaa ’\u0026gt;’ abc ’)’ ] Variables # Declaration: var=69 # no spaces # if uninit, `echo $var` returns empty string (`echo ${var}` also works) # uninitialized var is not neccessarily an empty string by default Double Quotes $ f=`Stupid Example.js` $ ls $f # equiv to `ls Stupid ; ls Example.js` $ ls \u0026#34;$f\u0026#34; # equiv to `ls \u0026#34;Stupid Example.js\u0026#34;` # double quotes and variables($) are \u0026#39;special\u0026#39; String Testing # Non-Empty String: [ -n string ] Empty String: [ -z string ] Tricky: # suppose {V} is an empty string [ -n $V ] # doesnt work (interpreted as `[ -n ]`) [ -n \u0026#34;$V\u0026#34; ] # works! Case Matching # eg.\ncase \u0026#34;$var\u0026#34; in *.py) # Case python rm \u0026#34;$var\u0026#34; ;; *.c | *.sh | myscript) # Case C, Shell, myscript echo w00t \u0026#34;$var\u0026#34; ;; *) # else echo meh \u0026#34;$var\u0026#34; esac # end statement "},{"id":7,"href":"/notes/CSCC01/","title":"CSCC01","section":"Home","content":" CSCC01 - Intro to Software Engineering # Semester Taken: Fall 2020\nProgramming Languages: N/A\nCourse Description:\nIntroduction to software development methodologies with an emphasis on agile development methods appropriate for rapidly-moving projects.\nCore Topics:\nBasic software development infrastructure Requirements elicitation and tracking Prototyping; basic project management Basic UML Introduction to software architecture Design patterns Testing. Course Page\n"},{"id":8,"href":"/notes/CSCC24/Week-3/","title":"Week 3","section":"CSCC24","content":" Week 3 # Higher-Order Procedures # Procedures as returned values.\nAll modern functional PLs manipulates functions as values.\n; f, g are functions and (f (g)) is well-defined (define (compose f g) (lambda x (f (g x)))) ; returns the composed function map # Usage:\n(map proc l1 l2 ... ln) proc: n-ary procedure (expects n arguments)\nl1, ..., ln: lists of length m\nReturns (e1 ... em) where ei = proc(l1[i], l2[i], ... ln[i])\nExample:\n(map (lambda (x y) (+ x y)) \u0026#39;(1 2 3) \u0026#39;(6 5 4)) ; ==\u0026gt; \u0026#39;(7 7 7) fold # Usage:\n(foldr op id lst) (foldl op id lst) op: binary procedure\nlst: list of arguments\nid: identity element; always used\nReturns result of applying op to elements in lst\nfoldr: right-associatively foldl: left-associatively Examples:\n(foldr op id \u0026#39;(e1 ... en)) ;\t==\u0026gt; (op e1 (op e2 (op ... (op en id)))) ; where (op en id) = id (foldr + 0 \u0026#39;(1 2 3)) (+ 1 (foldr + 0 \u0026#39;(2 3))) (+ 1 (+ 2 (foldr + 0 \u0026#39;(3)))) (+ 1 (+ 2 (+ 3 (foldr + 0 \u0026#39;())))) (+ 1 (+ 2 (+ 3 0))) (+ 1 (+ 2 3)) (+ 1 5) 6 ; (append xs ys) (define (fappend xs ys) (foldr cons ys xs)) ; (map f xs) (define (fmap f xs) (foldr (lambda (x r) (cons (f x) r)) \u0026#39;() xs)) "},{"id":9,"href":"/notes/CSCB09/Week-4/","title":"Week 4","section":"CSCB09","content":" Week 4 # Shell Scripting # Running Scripts # $ sh myscript Alternatively, in your myscript.sh file, do the following\n#! /bin/sh chmod u+x myscript # sets \u0026#39;executable\u0026#39; flag on the file and run it with\u0026hellip;\n$ ./myscript Positional Parameters # $ ./myscript foo bar baz # come after filename, spaced $#: Number of arguments (3) $n: Parameter name (n is the number) eg. $1 = \u0026quot;foo\u0026quot; $*: One string with all parameter names \u0026quot;foo bar baz\u0026quot; $@: Comma seperated strings for each parameter name \u0026quot;foo\u0026quot;, \u0026quot;bar\u0026quot;, \u0026quot;baz\u0026quot; shift: Shifts Positional Parameters by 1 $# = 2 $1 = \u0026#34;bar\u0026#34; $2 = \u0026#34;baz\u0026#34; $* = \u0026#34;bar baz\u0026#34; # good for looping over parameters Example Script # #! /bin/sh chmod u+x pydelete # sets exec flag on pydelete.sh dryrun= verbose= while [ $# -gt 0 ]; do # while the number of params \u0026gt; 0 case \u0026#34;$1\u0026#34; in # takes the first parameter/argument -n) # if the argument is -n dryrun=y # sets the program as a dryrun \u0026#34;doesnt actually delete files\u0026#34; ;; -v) verbose=y # sets the program as verbose \u0026#34;deletes verbosely\u0026#34; ;; *) # as soon as there is another different parameter, the while loop breaks break esac # end of case shift # basically $# -= 1 done # end of while for f in \u0026#34;$@\u0026#34; ; do # for each parameter (after remove the arguments) case \u0026#34;$f\u0026#34; in # begins case *.py) # if the file is .py extension [ -n \u0026#34;$verbose\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;deleting $f\u0026#34; # if user used `-v` [ -z \u0026#34;$dryrun\u0026#34; ] \u0026amp;\u0026amp; rm \u0026#34;$f\u0026#34; # if user did not use `-n`, then it will delete ;; *) # any other file extension [ -n \u0026#34;$verbose\u0026#34; ] \u0026amp;\u0026amp; echo \u0026#34;not deleting $f\u0026#34; # if the user set verbose esac # end of case done # end of for exit # Terminates script and/or shell process Useful for: Early exit Non-zero exit codes: exit 1 Functions # Example definition myfunc() { echo \u0026#34;Hot diggity dog\u0026#34; } Example function call myfunc foo bar baz # positional parameters are now function arguments Returning return # early return return 1 # with exit code Variable Scopes Local variables myfunc() { local x y z x=69 y=351 z=$(expr $x + $y) echo $z } Dynamic Scoping $ cat myscript x=0 printnum() { local x echo $x } func1() { local x x=1 printnum } func2() { local x x=2 printnum } func1 ; func2 $ sh myscript 1 # from func1 2 # from func2 Feeding multi-line text into stdin # $ cat \u0026lt;\u0026lt; EOF # could be any string ﹥Hi I\u0026#39;m Navinn ﹥Variables also work \\$x=$x ﹥EOF # tells shell its the end of file Hi I\u0026#39;m Navinn Variables also work $x=42069 if end-marker is declared in quotes, $ is no longer special. Command Substitution # Run a command and take its stdout in-place for i in $(cat myfile) ; do # if wrapped in double-quotes, `cat myfile` is just one string instead of multiple echo \u0026#34;$i\u0026#34; done Environment Variables # Every process has a collection of environment variables eg. PATH, CLASSPATH, USER, PWD, PS1 etc. Convention is all caps Same syntax: eg. $PATH, PATH=... printenv: prints current env. vars. $ printenv HOME=/Users/home PWD=/Users/home/desktop PATH=/usr/local/cms/jdk1.8.0_31/bin:/usr/bin:/bin # colon-seperated list of directories ANDROID_HOME=/Users/home/Library/Android/sdk SHELL=/bin/bash ... Creating a new env. var. is different LOGNAME=navn # same init export LOGNAME # different export LOGNAME=navinn # alt File Attributes # General Attributes # Size Type Last modified time Last access time Last change time Owning user Owning group \u0026hellip; Unix Account Organization # Two main accounts: user, groups\nuser: the user (home, navinn etc.) groups: A user can be in multiple groups groups command displays which groups the current user is in $ groups staff everyone localaccounts _appserverusr admin # ... Permission Flags # Each file is assigned one owning user and one owning group. Default is the user who created it and their default group\nPermissions: take the form rwxrwxrwx r: read w: write x: execute File permission notation $ ls -l rwxr-x--x 1 home staff 73966 1 Jun 23:07 myscript.sh # perms. user group bytes date_created name # rwxr-x--x: user gets `rwx`, users in group get `x`, other users get `x` For directories:\nr: list files in the dir ls w: add, delete, modify files in the dir x: cd into dir., use paths mentioning dir To Check Perms on a file/dir:\n[ -r path ] # exists and readable by current user [ -w path ] # exists and writable [ -x path ] # exists and executable Changing Ownership/Permission\nchown user path1 path2 # changes the owning user for both paths chown user:group path1 path2 # changes owning user and owning group chown :group path1 path2 # changes group chmod u=rw,g=r,o= path1 path2 # changes specific perms (u=user, g=group, o=other) find # Literally finds a path and operates on selected files with automatic recursion\n$ find path ...expression For each given path, recurse down and pick out a file based on the expression\neg. Find python files, print their path, then delete\n$ find . -name ’*.py’ -exec rm ’{}’ ’;’ -print "},{"id":10,"href":"/notes/CSCC24/","title":"CSCC24","section":"Home","content":" CSCC24 - Principles of Programming Languages # Semester Taken: Winter 2021\nProgramming Languages: TBA\nCourse Description:\nMajor topics in the design, definition, analysis, and implementation of modern programming languages.\nStudy of programming paradigms:\nProcedural (C, Java, Python) Functional (Scheme, ML, Haskell) Logic programming (Prolog, Mercury) Course Page\n"},{"id":11,"href":"/notes/CSCC24/Week-4/","title":"Week 4","section":"CSCC24","content":" Week 4 # Local Bindings # Creating variables with a local scope, and bind them to the result of expressions\nScope: Visibility of variables let # Usage:\n(let ([var1 expr1] ... [varn exprn]) body) expr1, ..., exprn are evaluated in an undefined order, has the appearance of running in parallel\nThe scope of var1, ..., varn is body\nNote: vari is not in the scope of [varj, exprj] when i != j, due to the parallel nature\nExample:\n(define double-triple (x) (let ([double (* 2 x)] [triple (* 3 x)]) (list double triple))) let* # Same usage as let\nexpr1, ..., exprn are evaluated sequentially left to right\nThe scope of vari is everything to the right of vari\nExample:\n(define double-triple (x) (let* ([double (* 2 x)] [triple (+ x double)]) (list double triple))) letrec # Same usage and order of evaluation as let\nThe scope of vari is the entire letrec\nExample:\n;; Tail Recursion - allows var to be called in expr (define len-of-list (lst) (letrec ([length-tail (lambda (lst len) (if (empty? lst) len (length-tail (rest lst) (+ len 1))))]) (length-tail lst 0))) ;; another recursive example (letrec ([my-even? (lambda (x) (if (eq? x 0) #t (my-odd? (- x 1))))] [my-odd? (lambda (x) (if (eq? x 0) #f (my-even? (- x 1))))]) ; allows my-even? to be in scope when called in my-odd? ; and vice versa (if (my-even? 4) 1 0)) "},{"id":12,"href":"/notes/CSCB09/Week-5/","title":"Week 5","section":"CSCB09","content":" Week 5 # Programming in C # Memory Model # Array of bytes, \u0026ldquo;Addresses\u0026rdquo; are indexes Variables may occupy several consecutive bytes, its address refers to the first occupied byte \u0026ldquo;Pointer\u0026rdquo;: Variable/parameter that stores an address int i = 69; // Suppose i was occupying bytes 45 to 74 int *p = \u0026amp;i // 45 Memory Regions # Text: Stores code Pointers pointing to functions point here Global: Stores global variables Stack: Used for function calls Stores local variables Auto allocation and deallocation Heap: Manual allocation (malloc(), calloc()) and deallocation (free()) Used for dynamic data outside of functions Global Variables # Two types of varaibles int pubVar = 10; // top-level public global variable int f() { static int privVar = 1010; // function private global variable pubVar++; privVar++; ... } Integer Types # All possible combinations: {signed, unsigned $\\times$ char, short, int, long, long long} Byte size depends on platform eg. x86-64 Type Size (bytes) char (default signed) 1 short 2 int 4 long 8 long long 8 Integer Literal Notation # Literal Type 3 int c char 3U unsigned int 3L long 3UL unsigned long 3LL long long 3ULL unsigned long long printf(\u0026#34;%lu\\n\u0026#34;, 3UL) // Good usage printf(\u0026#34;%lu\\n\u0026#34;, 3) // Bad since 3 is `int` Type Casting with Numbers # Larger size type to smaller: Automatic conversion Lose some information in a natural way (double to int removes decimal place) Better to explicitly typecast: double d = 420.69; int i = (int)d; Smaller size type to larger: Automatic conversion Completely lossless Implicit Number Promotion # The smaller operand type gets promoted to the larger operand type Note: char and short are always promoted to int eg. // Suppose the following double d = 65.0; int i = 65; char c = \u0026#39;A\u0026#39;; /** i / c -\u0026gt; promotes c to `int` and preforms integer division d / j -\u0026gt; promotes j to `double` and preforms floating-point division **/ Enumeration Types # New \u0026ldquo;types\u0026rdquo; and integer constant names enum rps { ROCK, // ROCK = 0 PAPER, // PAPER = 1 SCISSORS // SCISSORS = 2 }; enum coin { HEAD, // HEAD = 0 TAIL // TAIL = 1 }; enum rps a = PAPER; // a = 1 enum coin c = HEAD; // c = 0 \u0026ldquo;Types\u0026rdquo; are simply int, are mixable and not checked Practically useful for meaningful names only Union Types # Overlapping \u0026ldquo;fields\u0026rdquo; that share the same space union myUnion { // sizeof(myUnion) = \u0026#34;largest field size\u0026#34; = 4 (for this example) unsigned short s; unsigned int i; unsigned char b[4]; }; union myUnion u; // can use u.s, u.i, u.b[j] etc. Use Cases: High-level: Data has multiple mutually exclusive cases Low-level: Store an int in i Read b[0] to b[3] to discover how i splits Tagged Union Idiom # Example: Suppose you wanted an array that holds both int and double Idiom: Make an outer struct struct int_or_double { enum { INT, DOUBLE } tag; // remembers which case you are in union { // shares the same space in memory regardless of int or double int i; // case value is an int double d; // case value is a double } data; }; struct int_or_double a[10]; // as wanted Type Alias with typedef # Very general, i.e can use with struct, enum, int, double etc. typedef struct node { int i; struct node *next; } nodetype; // use `nodetype` instead of `struct node` Cannot use the same typedef name for more than one thing typedef coin { HEAD, TAIL } coin; typedef int coin; // illegal since `typedef coin` is already defined "},{"id":13,"href":"/notes/CSCC43/","title":"CSCC43","section":"Home","content":" CSCC43: Introduction to Databases # Semester Taken: Fall 2021\nProgramming Languages: SQL\nCourse Description:\nIntroduction to database management systems.\nThe relational data model Relational algebra Querying and updating databases: the SQL query language Application programming with SQL Integrity constraints, normal forms, and database design Elements of database system technology: query processing, transaction management Course Page\n"},{"id":14,"href":"/notes/CSCC24/Week-5/","title":"Week 5","section":"CSCC24","content":" Week 5 # Tail Recursion # Consider the following function\n(define (len xs) (if (empty? xs) 0 (+ 1 (len (rest xs))))) ; trace (len \u0026#39;(1 2)) (+ 1 (len \u0026#39;(2))) (+ 1 (+ 1 (len \u0026#39;()))) (+ 1 (+ 1 0)) (+ 1 1) 2 The space complexity for this is precisely $O(n)$, where $n$ is the length of xs.\nFor a small enough stack space, and large enough list, this function can result in a Stack Overflow. To fix this issue, we must implement this function using tail recursion.\n(define (len xs) (local [(define (len* xs l) ; accumulator (if (empty? xs) l (len* (rest xs) (+ 1 l))))] (len* xs 0))) ; trace (len \u0026#39;(1 2)) (len* \u0026#39;(1 2) 0) (len* \u0026#39;(2) 1) (len* \u0026#39;() 2) 2 Tail-Call Optimization # A language can implement Tail-Call Optimization, where no stack is required\nSome languages (Scheme) require tail-recursion to be implemented, some don\u0026rsquo;t (Python)\nTypes of Recursion # Linear Recursion: At most one recursive call occuring in the body of the function\nTail Recursion: Recusive call is the last evaluated step in the body of the function\nFlat Recursion: Only recurse on the top level of the list\nDeep (or Tree) Recursion: Recurses on all items in the list (sublists)\nMutual Recursion: Pair of functions, that call each other, not themselves\nContinuation-Passing Style (CPS) # Every well-formed subexpression has a continuation; what needs to be done to complete the expression\nConsider the following expression\n(* 2 (+ 4 5)) The continuation of (+ 4 5) would be (+ 2 []) or in Scheme\n(lambda (v) (+ 2 v)) Every well-formed sub-expression in the evaluation corresponds to a point in the evaluation of the program, and v.v\nRewriting the above len function using CPS\n(define (len xs) (local [define (len* xs k) ; k -\u0026gt; continuation function (if (empty? xs) (k 0) (len* (rest xs) (lambda (v) (k (+ 1 v)))))] (len* xs identity))) ; trace (len \u0026#39;(1 2)) (len* \u0026#39;(1 2) identity) ; identity = (lambda (v) v) (len* \u0026#39;(2) (lambda (v1) (identity (+ 1 v1)))) (len* \u0026#39;() (lambda (v2) ((lambda (v1) (identity (+ 1 v1))) (+ 1 v2)))) 2 ; computed all in one-step ((lambda (v2) ((lambda (v1) (identity (+ 1 v1))) (+ 1 v2))) 0) ; v2 = 0 ((lambda (v1) (identity (+ 1 v1))) 1) ; v1 = 1 (identity 2) 2 "},{"id":15,"href":"/notes/CSCB09/Week-6/","title":"Week 6","section":"CSCB09","content":" Week 6 # Programming in C (cont.) # File I/O in C # Content can be accessed as a steam (seq. read/write) File functions work with FILE * FILE: Type rep. stream state, definition varies by platform, most likely a struct Open # FILE *fopen(const char *filename, const char *mode) Modes: \u0026quot;r\u0026quot;, \u0026quot;w\u0026quot;, \u0026quot;a\u0026quot;, \u0026quot;r+\u0026quot;, \u0026quot;w+\u0026quot;, \u0026quot;a+\u0026quot; Read, Write, Append, Read \u0026amp; Write, Write \u0026amp; Read, Append \u0026amp; Read Appending and Writing to a file will create a new one if the file does not exist Reading to a non-existent file will cause an error (returns NULL) Close # int fclose(FILE *stream) Close stream when finished with the file Returns 0 if success, EOF if error Why should you close the file as soon as you\u0026rsquo;re finished: There is a limit on how many streams are open per process Writing may be buffered until closing No two process can open the same file (Windows only) Formatted I/O # // printf and scanf but for a given stream int fprintf(FILE *stream, const char *format, ...) int fscanf(FILE *stream, const char *format, ...) printf is just fprintf but stdout is specified scanf(format, args) == fscanf(stdin, format, args) //same for scanf You do not need to manually close stdin, stdout, stderr streams Character and String I/O # One single character: returns EOF if error or not found int putchar(int c) /* stdout */ int putc(int c, FILE *stream) int getchar(void) /* stdin */ int getc(FILE *stream) String: int fputs(const char *string, FILE *stream) // does not put a newline at the end char *fgets(char *dest, int n, FILE *stream) // Reads at most (n-1) chars or until (and including) newline Arbitrary Data I/O # size_t fread(void *dest, size_t s, size_t n, FILE *stream) size_t fwrite(const void *data, size_t s, size_t n,FILE *stream) Reads/Writes n items, each of s bytes Returns how many items have been read/written Potential Use Cases: A whole array struct Raw bytes (array of unsigned char) Error versus End-of-Stream Disambiguation # getc, scanf return EOF on error fgets returns NULL fread returns \u0026lt; n int feof(FILE *stream) // returns true if end-of-stream int ferror(FILE *stream) // returns true if error void clearerr(FILE *stream) // clears end-of-stream and error status Error Information # #include \u0026lt;errno.h\u0026gt; Global variable stores error reason of most recent error int errno; Many possible values: ENOENT // File does not exist EACCESS // No permission EDOM // sqrt(-3.0) Usually, we just use perror to print the error message to stderr: void perror(const char *prefix) Buffering # C delays file writing Accumulates data in a buffer until it is large, then requests the kernel to write that chunk Also hastens reading Requests kernel to read a large chunk into the buffer, then serves read requests from said buffer Buffer Operations # int fflush(FILE *stream) // Returns 0 if success, EOF if error Writes the buffer for the output stream Clears the buffer for the input stream int setvbuf(FILE *stream, char *buf, int mode, size_t n) mode meaning _IOFBF full buffering _IOLBF line buffering _IONBF no buffering Default Buffering of stdin, stdout, stderr # If Terminal:\nstdin: line buffer stdout: no buffer stderr: complicated If in a file or pipelined:\nstdin: full buffer stdout: no buffer stderr: full buffer Seeking # Ask current position: long int ftell(FILE *stream) // returns -1L if error Seek: int fseek(FILE *stream, long i, int origin) // non-zero return if error origin go to i bytes from SEEK_SET beginning SEEK_END end SEEK_CUR current position "},{"id":16,"href":"/notes/CSCC69/","title":"CSCC69","section":"Home","content":" CSCC69: Operating Systems # Semester Taken: Winter 2023\nProgramming Languages: C\nCourse Description:\nPrinciples of operating systems.\nThe operating system as a control program and as a resource allocator. The concurrency problem: synchronization, mutual exclusion, deadlock The concept of a process Process scheduling Threads Memory management File systems Course Page\n"},{"id":17,"href":"/notes/CSCC24/Week-6/","title":"Week 6","section":"CSCC24","content":" Week 6 # Pure Functional Languages # Programs: collections of functions Execution: view as evaluation Referential Transparency # The value of applying a function is independent of its context\nConsider the program\n# Top part of program f(a, b, c) # well-formed expression # Bottom part of program A program exhibits Referential Transparency, if it behaves the exact same, when the expression f(a, b, c) is replaced with its value.\nThe expression does not depend on the global state of computation. i.e. only on f, a, b, and c.\nNo Assignment Statements # Variables retain their acquired value until the end of the evaluation\nIn an imperative language\u0026hellip;\nint x = 42; x = x + 1; // goes to the locker that stores x // increments the value // and overwrites the locker In a functional language\u0026hellip;\n(define x 42) (define x (+ x 1)) ; just an association ; points to a different locker No Side Effects # Functions do not change things outside of its own scope\nlst = [1, 2, 3] # no side effects def good-sum(lst, x): return sum(lst) + x # value of lst outside of bad-sun\u0026#39;s scope is changed def bad-sum(lst, x): lst.append(x) return sum(lst) # lst is now [1, 2, 3, x] Closures # A combination of a function, and the lexical environment within which the function was declared\nConsider this closure from a functional PL\nconst addThree = (x, y) =\u0026gt; x + y + z // 1. get values x, y from the lexical scope (parameters) // 2. get z from the global scope // 3. return the sum of x, y, z Lexical Environment: reference to its surrounding state\nLexical Scope: uses the location of where the variable has been declared in the source code, in order to evaluate it\ni.e. x and y are declared when calling addThree Functions have access to variables declared in their outer scope\ni.e. the outer scope of addThree is the global scope, which is where it will find z Only upon evaluation time, will the value of z be determined, meaning addThree basically is a recipe of steps to follow every time it is called.\n"},{"id":18,"href":"/notes/CSCB09/Week-7/","title":"Week 7","section":"CSCB09","content":" Week 7 # Programming in C (cont.) # Compiler and Linker Stages # myProgram.c $\\to$ Compiler $\\to$ Machine Code (myProgram.o) $\\to$ Libraries + Linker $\\to$ Executable\nmyProgram.o is the object code file Libraries: where methods from stdio.h/stdlib.h come from Linker: Merges object files and libraries into one executable gcc serves as a convenient linker frontend C Compiler Stages # The C compiler futher breaks down into:\nPre-processor: For # directives, determines the actual C code seen by the compiler proper $ gcc -E ... # to see what the pre-processor actually does Compiler proper: Translates C code into machine code $ gcc -c ... # creates the object file with the machine code Pre-processor Directives # #define macros Textual substitution #define FINAL_COURSE_MARK 100 // to define a macro #undef FINAL_COURSE_MARK // to remove the macro #include for header files Header files usually contain Macro defintions Types of exported functions and global variables Implementation #include \u0026lt;foo.h\u0026gt; // looks for file in system-wide places (\u0026#39;/usr/include\u0026#39;) #include \u0026#34;foo.h\u0026#34; // looks for file among user source code Conditional Compilation #ifdef DEBUG_FLAG fprintf(stderr, \u0026#34;x=%d\\n\u0026#34;, x); #endif Common technique for debugging code Also useful to check current OS (Windows/Linux) Modularity and Seperate Compilation # Keep closely-related code in the same files Key idea/principle for good software design Easier to modify and recompile one small file then one large file Example: // Rectangle Struct with area function typedef struct rect_struct { double width, height; } rect; double calcArea(const rect *r) { return ... //implementation goes here } // Linked-List Struct typedef struct ll_node_struct { rect r; ll_node_struct *next; } listNode; // Main Program int main() { rect r; listNode *newNode = (listNode *)calloc(1, sizeof(listNode)); printf(\u0026#34;%d\\n\u0026#34;, calcArea(\u0026amp;r)); ... } This large file can be split into smaller more cohesive files rect.h: includes type definition for rect_struct and calcArea() prototype rect.c: includes calcArea() implementation/definition llnode.h: includes ll_node_struct definition main.c: includes the main program # only compile to machine code if it is necessary (new changes) $ gcc -c rect.c $ ... # takes the machine code and makes the executable $ gcc rect.o bb.o mainprog.o -o mainprog Header Files # A seperate file specifically for macro/type definitions and function prototypes Compiler wants to see the definition and prototype but you do not want to manually copy it to multiple files Note: It is illegal to see a type definition twice when compiling Solution: Using Conditional Compilation to check if the header file was already included from another file, and including if it hasn\u0026rsquo;t #ifndef _FOO_H // if _FOO_H has been defined already, skip #define _FOO_H // if it has not been defined, then define it typedef struct node { int i; struct node *next; } node; #endif Makefiles and make # Most Basic Clause/Rule # Form:\nTARGET : PREREQUISITES RECIPE Example:\nbb.o : bb.c bb.h rect.h # if `bb.o` is missing or older than `bb.c`, `bb.h` and `rect.h` gcc -c bb.c # then run this command (compile to machine code) If there are multiple rules in a Makefile:\nmake triggers the first rule (which may trigger other subsequent rules) Order does not matter otherwise Customary to write first rule as\nall : myexe1 myexe2 myexe3 # triggers other rules to build each myexe file .PHONY : all # means `all` is just a label, not an actual target file File Clean Up # Customary to add clean : # no prerequisites rm -f *.o myexe1 myexe2 myexe3 .PHONY : clean Run make clean to invoke this target rule Variables # Defining variables from within a Makefile CFLAGS = -g Setting variables outside of a Makefile make CFLAGS=\u0026#39;-g -DMY_DEBUG_FLAG\u0026#39; # overrides CFLAGS from within Makefile Using a variable gcc $(CFLAGS) -c bb.c Automatic Variables and Pattern Rules # Non-example\nmainprog : mainprog.o bb.o rect.o gcc -g mainprog.o bb.o rect.o \\ -o mainprog mainprog.o : mainprog.c bb.h rect.h gcc -g -c mainprog.c bb.o : bb.c bb.h rect.h gcc -g -c bb.c rect.o : rect.c rect.h gcc -g -c rect.c Using pattern rules\nmainprog : mainprog.o bb.o rect.o gcc -g $^ # all prereqs -o $@ # target %.o : %.c # any `.o` target with `.c` prereq gcc -g -c $\u0026lt; # build the first prereq Automatic prerequisite listing\n$ gcc -MM mainprog.c bb.c rect.c mainprog.o : mainprog.c rect.h bb.h bb.o : bb.c rect.h bb.h rect.o : rect.c rect.h # copy the output to your Makefile "},{"id":19,"href":"/notes/CSCC24/Week-7/","title":"Week 7","section":"CSCC24","content":" Week 7 # Type Systems # Type: Name of a set of values and operations which can be performed on the set\nAlternate: Collection of computational entities that share some common property What is or is not a type, is language dependent A PL is Type Safe if it won\u0026rsquo;t execute a function if it\u0026rsquo;s not applicable to the arguments\nType Checking:\nStatic: at compile-time Dynamic: at run-time Motivation # Catch all errors at compile-time (impossible, even in theory) Solution: Guarantee to catch only a certain class of errors Type safety without explicit declaration Benefits of Type Systems # Easier to debug\nStatic analysis - information obtained at compile-time\nCan compile quicker/more optimized code (save computation, limit memory)\nCan be used to prove correctness\nSelf documenting code\nType Notations (Haskell) # Int: integers\nInteger: unbounded integers\nFloat, Double: floating point numbers; single and double precision\nChar: characters\nBool: boolean values\nTypeA -\u0026gt; TypeB: function that takes in TypeA and returns TypeB\nEvery function accepts exactly one argument (can be a tuple) (): \u0026lsquo;unit\u0026rsquo; - empty tuple\n(Type1, Type2, ..., TypeN): tuples of Types (heterogeneous)\n[Type]: list whose elements are all Type\nParametric Polymorphism (Type Variables) # When the type cannot be inferred\nConsider the Haskell function\nprompt\u0026gt; func (x, y, z) = if x then y else z prompt\u0026gt; :t func func :: (Bool, p, p) -\u0026gt; p p is a Type Variable - any valid type, func is a Polymorphic function\n$\\alpha \\to \\alpha$: for every valid type $\\alpha$\ni.e. when y, z is of type Int, $\\alpha$ is instantiated to Int Example:\nprompt\u0026gt; swap (x, y) = (y, x) prompt\u0026gt; :t swap swap :: (t, t1) -\u0026gt; (t1, t) $(\\alpha, \\beta) \\to (\\beta, \\alpha)$: for valid types $\\alpha, \\beta$ Type Checking # The process of verifying + enforcing type constraints\nDynamic Type Checking # Performed at run-time\nSlower execution\nMore flexible/freedom: easier to re-factor\nStatic Type Checking # Performed at compile-time\nFaster execution\nArguably safer and more elegant/modular programs\nMore compiler optimization\nProgrammers may end up writing worse code to get around static type checkers\nconst o: Type = { foo: 42 }; (o as any).bar = baz; Explicit Static Typing # Declaring type annotations in the code\n// explicitly declaring main to be String[] -\u0026gt; void public static void main(String[] boostMyMark) { int x; // declaring x to be of primitive int } Type Inference # Deduce types without explicitly declaring them\n(define (func x) (if (empty? x) 0 (length x))) ; can deduce that x is a pair, and return value is an int "},{"id":20,"href":"/notes/CSCB09/Week-8/","title":"Week 8","section":"CSCB09","content":" Week 8 # File System with C # i-nodes # The file system has an array of \u0026lsquo;i-nodes\u0026rsquo;\nEvery file and directory is identified using an i-node\nAn i-node stores:\nType: Regular file, dir, link, device, socket, etc. Permissions The owning user and owning group (numerical ids) Size Timestamps (created, last modified) Which disk blocks are used Other metadata You can get most metadata by using the stat command\nDesktop ~$ stat 2942805567 687 crw--w---- 1 home tty 268435456 0 \u0026#34;Jul 18 13:29:59 2020\u0026#34; \u0026#34;Jul 18 13:29:59 2020\u0026#34; \u0026#34;Jul 18 13:29:59 2020\u0026#34; \u0026#34;Dec 31 19:00:00 1969\u0026#34; 131072 0 0 (stdin) Directories # Stores the mapping of filenames to i-node numbers Data structure varies by system The C functions opendir, readdir, closedir can access directories portably Linking # Unlinking # Also known as deleting a file by its filename When deleting a file, the kernel will: Decrease the reference count of filenames referring to the i-node number If the reference count is now 0, the disk space and the i-node is freed up System call for delete is unlink Hard link # Creates a literal link between 2 files Useful when you want to access a file in an inaccessible directory ln can create another filename that has the same i-node number as an existing file $ ln path/to/file1 path/to/file2 # this creates a \u0026#39;hard link\u0026#39; The hard link between 2 files will not be broken if one of the file names change or even get deleted Hard linking dirs is disallowed unless implementing special dirs like . and .. Symbolic Linking (Symlink) # Special file that simply stores a path (sort of like a shortcut) Most system calls and C library functions/programs follows symlinks Create a symlink using ln -s Bitwise Operators in C # Bitwise AND (\u0026amp;), OR (|), NOT (~), XOR (^) Left Shift (\u0026lt;\u0026lt;) and Right Shift (\u0026gt;\u0026gt;) Example: unsigned char a = 10001001; // 8-bit binary unsigned char b = 00000011; a \u0026amp; b = 00000001 // logical AND on each bit a | b = 10001011 // logical OR on each bit a ^ b = 10001010 // XOR on each bit ~ a = 01110110 // flips all bits a \u0026lt;\u0026lt; 1 = 00010010 // shifts all bits 1 to the left b \u0026gt;\u0026gt; 2 = 00000000 // shifts all bits 2 to the right File Mode Bits # st_mode bitwise layout\nFile Type Permissions ---------------- ------------------------------------------------ | - | - | - | - | U | G | T | R | W | X | R | W | X | R | W | X | ----------- ----------- ----------- User Group Other Macros for checking individual permissions:\nS_IRUSR: Binary 0000 100 000 000 S_IWUSR: Binary 0000 010 000 000 S_IXUSR: Binary 0000 001 000 000 System Calls for File I/O # int open(const char *path, int flags); int open(const char *path, int flags, int mode); flags: O_WRONLY, O_RDONLY, O_RDWR, O_EXCL, O_TRUNC, O_APPEND; // Combine flags with bitwise OR size_t read(int fd, void *buf, size_t count); size_t write(int fd, void *buf, size_t count); off_t lseek(int fd, off_t offset, int origin); origin: SEEK_SET, SEEK_CUR, SEEK_END; // Takes another fdt entry to the same open file table entry int dup(int oldfd); // takes fdt entry to be `newfd` and duplicates `oldfd` to it int dup2(int oldfd, int newfd); int close(int fd); File Descriptor (fd) # Every process has a finite \u0026lsquo;File descriptor table\u0026rsquo; for opened files The File Descriptor is an array index into it Example of file descriptors: 0: stdin 1: stdout 2: stderr open() and dup() consumes the lowest-number free entry close() frees entries "},{"id":21,"href":"/notes/CSCC24/Week-8/","title":"Week 8","section":"CSCC24","content":" Week 8 # Pattern Matching # Input parameters for a function are matched from top down\nlen :: [a] -\u0026gt; Integer len [] = 0 -- if the input is null, returns 0 len (_ : xs) = 1 + len xs The _ represents a \u0026lsquo;do not care\u0026rsquo; value, as its not bound to a variable\nthis.handle((_, event) =\u0026gt; ...); Value Matching in Haskell # reverse xs = rev xs [] where rev [] rs = rs rev (y : ys) rs = rev ys (y : rs) abs x = if x \u0026gt;= 0 then x else =x abs x | x \u0026gt;= 0 = x | otherwise = -x Type Classes # Offer a controlled approach to overloading\nType Classes basically have lots of types, where each type has implementations of those defined in the class\nPredifined type classes in Haskell are Eq, Ord, Show, etc.\nConsider the type for the number 69\n69 :: Num a -\u0026gt; a --- can be Int, Integer, Rational ... This means:\nNum is the type class\nNum a is a type class constraint\n69 is of type a, where a is a type that belongs to the Num type class\nType Classes in a function # member :: Eq t =\u0026gt; (t, [t]) -\u0026gt; Bool --- type t belongs to the Eq class --- instances of type t have the (==) function member (y, (x:xs)) = y == x || member (y, xs) Defining Type Classes # class TypeClass a where classFunction :: a -\u0026gt; a --- could be any function --- optional: provide a default definition --- Example class Eq a where (==), (/=) :: a -\u0026gt; a -\u0026gt; Bool x == y = not x /= y x /= y = not x == y If possible, default definitions should be circular When making an instance of that class, only one needs to be implemented and the other is free Currying # Converting a multi-argument function into a sequence of evaluated functions, each with a single argument\nAlternative meaning: The action of cooking a traditional spicy dish of Indian and Oriental origin\nIn Haskell, each function takes exactly one argument Consider multiple ways of writing this function\nsum :: Num a =\u0026gt; (a, a) -\u0026gt; a sum (x, y) = x + y -- curried function sum\u0026#39; :: Num a =\u0026gt; a -\u0026gt; a -\u0026gt; a --- Num a =\u0026gt; a -\u0026gt; (a -\u0026gt; a) --- -\u0026gt; is right associative sum\u0026#39; x y = x + y sum\u0026#39; x = \\y -\u0026gt; x + y sum\u0026#39; = \\x -\u0026gt; \\y -\u0026gt; x + y :t (sum\u0026#39; x) (sum\u0026#39; x) :: Num a =\u0026gt; a -\u0026gt; a --- this is just a function "},{"id":22,"href":"/notes/CSCB09/Week-9/","title":"Week 9","section":"CSCB09","content":" Week 9 # Processes and Redirection # Launching a New Process # Clone the process pid_t fork(void); // child gets return value 0, parent gets child\u0026#39;s pid Both processes (child and parent) run the same code The child can switch to running another program execlp(path, arg0, arg1, ..., (char *)NULL); // \u0026#39;exec\u0026#39; family of system calls Things such as environment variables, pid, fd, current dir, etc. are preserved File descriptors can be closed before exec by marking them as close on exec Why seperate fork and exec? # Some use cases do not require exec The child process can do some prep before exec (file redirection and pipelining) The only \u0026lsquo;parent-less\u0026rsquo; process: init # fork is the only way to launch new processes As the kernel boots, it launches init, with pid 1 Process Commands # ps: List processes pgrep: Find processes by name, users, etc. top: Process list that periodically refreshes (think of it as a terminal task manager) htop: top, but better (more features) kill and pkill: Terminates a process if allowed (pkill finds like pgrep) Waiting for a Child process # pid_t wait(int *status); // status is for child\u0026#39;s exit code pid_t waitpid(pid_t pid, int *status, int options); // pid \u0026gt; 0 -\u0026gt; wait for the given child // pid == -1 -\u0026gt; wait for any child // options == WNOHANG -\u0026gt; don\u0026#39;t hang waiting Useful macros # Normal Termination: WIFEXITED, WEXITSTATUS Killed by signal: WIFSIGNALED, WTERMSIG, WCOREDUMP Stopped and continued by signal: WIFSTOPPED, WIFCONTINUED Termination of Parent before Child # If the child terminates first and the parent process is still running and does not call wait A \u0026lsquo;zombie\u0026rsquo; process of the child retains the entry of the child, but is not actually running If the parent terminates but the child is still running The child is now an \u0026lsquo;orphan\u0026rsquo; process and gets the parent pid of init If the child terminates and the parent calls wait Just regular termination and no \u0026lsquo;zombie\u0026rsquo; File Redirection # Before exec and fork, open the file Duplicate the file descriptor with dup2(curr_fd, new_fd) Close the file descriptor or request close on exec Then call exec Pipes # int pipe(int pipefd[2]); // creates a unidirectional pipe // pipefd[0] is for read end // pipefd[1] is for write end This is how shells do pipelining Usually, only one process at both ends For dup/dup2, stdout for write end, and stdin for read end. Close fds you do not need as soon as possible Kernel has a buffer for unread data if the write end is writing faster than the read end can handle Signals # How the kernel notifies processes of some events and severe errors\nExamples # Interupt (CTRL+C): SIGINT\nSuspend and Resume: SIGSTOP,SIGCONT\nChild died/suspended/resumed:SIGCHLD\nBroken pipe:SIGPIPE\nRequest for termination (shell ‘kill’ default):SIGTERM\nHard request for termination:SIGKILL\nIllegal memory access (two types:SIGBUS,SIGSEGV)\nApplication-specific:SIGUSR1,SIGUSR2\nSignal Life Cycle # Some events generate a signal Kernel tries to deliver the signal, and is pending until delivered Normal exec resumes if signal ignored or handler returns normally Shell Command: $ kill -SIGKILL 31337 $ kill -9 31337 System Calls: int kill(pid_t pid, int sig); int raise(int sig); // to self Signal Actions/Handlers # int sigaction(int sig, const struct sigaction *act, struct sigaction *oldact) sig: The signal type act: The new action you want oldact: The old action on fork: Signal actions cloned on exec: Handlers are replaced by default struct sigaction # struct sigaction { void *sa_handler(int sig); // pointer to handler function, SIG_IN, or SIG_DFL sigset_t sa_mask; // mask these signals when running handler int sa_flags; // options void *sa_restorer(void); // not for application use } sigset_t Operations # int sigemptyset(sigset_t *set); int sigfillset(sigset_t *set); // add all signals int sigaddset(sigset_t *set, int sig); int sigdelset(sigset_t *set, int sig); int sigismember(const sigset_t *set, int sig); sa_flags Flags # // If you install handler SA_NODEFER // don\u0026#39;t mask signal when running handler SA_RESETHAND // reset action to default before running handler SA_RESTART // auto-restart most syscalls before running handler // For SIGCHLD SA_NOCLDSTOP // don\u0026#39;t signal for child stop/cont. SA_NOCLDWAIT // don\u0026#39;t turn terminated child into zombie "},{"id":23,"href":"/notes/CSCC24/Week-9/","title":"Week 9","section":"CSCC24","content":" Week 9 # Infix Operators # Any curried function that takes 2 parameters can be converted, and vice versa\nx = 3 + 2 -- infix x = (+) 3 2 -- function y = elem 3 [1,2,3] y = 3 `elem` [1,2,3] -- infix Type Synonyms # Giving existing types an alias or new name\ntype NewType = OldType Useful for readability (String instead of [Char]) User Defined Data Types # General Syntax:\ndata NewType = Constructor1 Type1 | ... | ConstructorN TypeN |: represents union of the constructor and types\nexport Type NewType = Type1 | ... | TypeN TypeI: previously defined types\nConstructorI: creates a value of NewType type\nType can be omitted if the constructor does not need any argument (constants)\nEnumerated Types # data NumberType = Odd | Even --- only constants --- pattern matching on constructors mod2 :: Num a =\u0026gt; NumberType -\u0026gt; a mod2 Odd = 1 mod2 Even = 2 Variant/Union Types # data Text = Letter Char | Word [Char] --- either a single Letter or multiple letters (word) textLen :: Text -\u0026gt; Int textLen (Letter _) = 1 textLen (Word w) = length w Mutually Recursive Types # --- parametric polymorphism here data Tree a = Leaf a | Node a (Tree a) (Tree a) --- Tree is a type constructor --- Node is a curried value constructor countNodes:: Tree a -\u0026gt; Int countNodes (Leaf _) = 1 countNodes (Node _ left right) = countNodes left + countNodes right + 1 --- the structure of the function is the same as the data type --- i.e the recursive pattern is the same on both "},{"id":24,"href":"/notes/CSCB09/Week-10/","title":"Week 10","section":"CSCB09","content":" Week 10 # Sockets # Another way for two processes to communicate\nCharacteristics of Sockets # Has 2 sides: Server and Client Server: has a publishable address Client: contacts Server by published address Unrelated processes, even on different computers, can contact each other through sockets Socket Varieties # By Domain # Unix Domain: Local to the computer, address is a filename IPv4: Over the network, 32-bit address + 16-bit port number IPv6: 128-bit address and over the network By Abstraction Level # Datagram: Per packet Packet boundary preserved, packet order is not Unnoticed packet loss Stream: Network stack works hard to confirm, timeout, resend Preserves data order No packet boundary Stream Socket Workflow # Client # Call socket, creates the socket fd Fill in the address struct and use connect to connect to the server at the address Use the socket fd to communicate with the server Close the fd when done Server # Call socket to create the server socket fd Fill in the address struct and use bind to bind the sfd to the address Call listen Loop Call accept(sfd) to wait for client to connect, gets back a client fd Use the cfd to communicate with client, close when done Close the server socket fd if no longer waiting for clients Creating Sockets # int socket(int family, int type, int protocol); // returns socket fd \u0026gt; 0, -1 if error family: AF_UNIX, AF_INET (IPv4), AF_INET6 (IPv6) type: SOCK_DGRAM, SOCK_STREAM protocol: 0 IPv4 Addresses and Port struct # Adresses are 32-bit, and identifies network interfaces (computers) Each byte is seperated by dots. (ex 142.1.96.164) dig can look up IP Addresses from domain names by asking Domain Name Servers (DNS) Port struct: struct sockaddr_in { sa_family_t sin_family; // AF_INET in_port_t sin_port; // port, need to be in network byte order struct in_addr sin_addr; // IPv4 address, also in NBO }; struct in_addr { uint32_t s_addr; } Special addresses 127.0.0.1: Loopback (You can see this example when using localhost or any web server running on your computer) 0.0.0.0: Request binding to all network interfaces Endians and Network Byte Order # Big Endian (Network Byte Order): Left to right (ex. 772 in decimal = 03 04) Little Endian: Bytes are swapped (ex. 772 in decimal = 04 03) Use the library functions htonl (32-bit) and htons (16-bit) to convert from Little to Big Endian bind, accept and connect # int bind(int fd, const struct sockaddr *addr, socklen_t addrlen); sockaddr: sockaddr_in (IPv4), sockaddr_in6 (IPv6), sockaddr_un (UNIX) int accept(int fd, struct sockaddr *client_addr, socklen_t addrlen); Returns new socket cfd for talking to client client_addr will recieve the address of client int connect(int fd, const struct sockaddr *server_addr, socklen_t addrlen); Returns 0 if success, -1 on error fd can now talk to the server Broken Pipes # If one end of the pipe is closed before the other one, the processes gets SIGPIPE The default action is the processes gets killed Eg. $ uniq longFile.txt | head -1 # \u0026#39;head\u0026#39; end of pipe closes after the first line # but \u0026#39;uniq\u0026#39; is still running, hence the process just ends To override the default, set action to SIG_IGN (ignore) "},{"id":25,"href":"/notes/CSCC24/Week-10/","title":"Week 10","section":"CSCC24","content":" Week 10 # Short Circuiting # Defined in most if not all programing languages\nConsider the expression\nif x \u0026gt; 0 and 69 / x \u0026lt; 1: print(\u0026#39;hoe\u0026#39;) If x happened to be 0, evaluating 69 / x only would result in a division by 0 error The second comparison would only be evaluated if the first was not false i.e. if x was non-positive, then the rest wouldn\u0026rsquo;t be evaluated, acts as a \u0026lsquo;guard\u0026rsquo; Laziness # Only evaluate things when you need to\nHaskell is a lazy language, where everything is evaluated \u0026lsquo;at the very last second\u0026rsquo;.\nConsider this Java code\npublic void m(T x, T y) { // ... } // ... public static void main(String[] boostMyMark) { System.out.println(m(functionThatReturnsX(), functionThatReturnsY())); } When m is called, both functions that are the arguments are evaluated, this is Eager Evaluation.\nNow consider this Haskell code\nand\u0026#39; :: Bool -\u0026gt; Bool -\u0026gt; Bool and\u0026#39; False _ = False and _ x = x and\u0026#39; False (45 / 0 == -1) Due to the pattern matching in the definition of and', the second param will not be evaluated whatsoever, as in that pattern, its a do not care value.\nRecursive Values with Lazy Evaluation # nats = 0 : map (+1) nats --- [1, 2, 3, 4, ...] GHCI\u0026gt; length nats --- This will never be evaluated, since this is infinite GHCI\u0026gt; take 10 nats [0,1,2,3,4,5,6,7,8,9] --- Works! Since 10 is finite --- fibonacci numbers with list comprehension --- infinite definition, linear complexity fib = 0 : 1 : [x + y | (x, y) \u0026lt;- zip fib (tail fib)] "},{"id":26,"href":"/notes/CSCB09/Week-11/","title":"Week 11","section":"CSCB09","content":" Week 11 # Multiplexing Input and Output # Handling multiple input/output clients with select and epoll\nselect # int select(int n, fd_set *read_fds, fd_set *write_fds, NULL, struct timeval *wait_timeout); Blocks until fds are ready Returns 0 if reaches the timeout, else returns positive count if some fds are ready Modifies the given fd_sets, set them again before the next call fd_set: holds a set of file descriptors n: The highest fd you specify, + 1 fd_set functions # void FD_ZERO(fd_set *s); // empties the set void FD_SET(int fd, fd_set *s); // adds an fd to the set void FD_CLR(int fd, fd_set *s); // deletes an fd int FD_ISSET(int fd, fd_set *s); // queries if fd is in the set Limitations # Max size for fd_set usually 1024 Slow when using many file descriptors epoll # Only available on Linux\nint epoll_create1(int flags); Creates a new epoll instance Returns the file descriptor of this instance (epfd) flags: 0 or FD_CLOEXEC Instance only useful for other epoll functions int epol_ctl(int epfd, int op, int another_epfd, struct epoll_event *ev); Sets what the epoll instance will wait for Returns 0 if successful op: EPOLL_CTL_ADD, EPOLL_CTL_DEL, EPOLL_CTL_MOD ev: Events to wait for int epoll_wait(int epfd, struct epoll_event *evs, int n, int timeout); evs: Array to recieve events n: Length of events array Returns count of ready file descriptors struct epoll_event { uint32_t events; epoll_data_t data; }; You can combine events using bitwise operators\nevents:\nEPOLLIN: Ready to read\nEPOLLOUT: Ready to write\nEPOLLONESHOT: Monitor once only\nEPOLLET: Notify whenever changes from not-ready to ready\nEPOLLHUP: Other end of pipe/socket has closed\nEPOLLERR: Error condition\ntypedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; You store to this data when calling epoll_ctl Stored data gets returned when you call epoll_wait Usually store the fd, or a pointer to some struct to keep track of history "},{"id":27,"href":"/notes/CSCA48/final-review/","title":"Final Review","section":"CSCA48","content":" CSCA48 - Final Review # Winter 2020 # If you think it won\u0026rsquo;t work in C, it probably will.\nUnits 1 + 2: # Variables and lockers: # Uniquely numbered in increasing order, reserved only for the program using the box to store and access information in said box.\nThree ways to get a locker: # Variable Declaration int boostMyMark = 420; // locker has been created Return Values return boostMyMark; // new locker has been created, (copied value) Input Parameters void killAverage(double currentAvg, double midtermMark) {...} // lockers created to store currentAvg, midtermMark Arrays: # Fixed length and data type, consecutive boxes allocated in memory and are Passed-By-Reference for function calls.\nArray Declaration: # int crunchyArray[10]; // creates 10 consecutive boxes of memory Strings: # Array of chars End-of-string delimiter: \u0026lsquo;\\0\u0026rsquo; Pointers: # Just a variable (own space in memory), and stores a memory address in its locker of the same data type.\nExample:\nint num = 69; // creates locker int *p = \u0026amp;num; // the contents of (*) p gets the address of (\u0026amp;) num *(p) ++; // increments the contents of (*) p Arrays and Pointers: # Arrays get passed as a pointer in a function (index 0) int arr[3] = {1, 2, 3}; // 3 new consecutive lockers for arr int *p = NULL; // empty pointer // the following are equivalent p = \u0026amp;arr[0]; // p gets the address of the value of arr at index 0 p = arr; // p gets the start of arr Arrays can be iterated using an offset pointer or the indicies int nums[5] = {1, 2, 3, 4, 5}; int *p = nums; // \u0026amp;nums[0] for(int i = 0; i \u0026lt; 3; i++) { // the following are equivalent printf(\u0026#34;%d\\n\u0026#34;, nums[i]); // nums at index i printf(\u0026#34;%d\\n\u0026#34;, *(p + i)); // the contents of memory address (p + i) } Unit 3: # Dynamic Memory Allocation # Reserving space in memory so it doesn\u0026rsquo;t get released when your function exits\nInitializing on the stack versus the heap: # int crunchyFunction() { int stackNum = 69420; // allocates memory on the stack int *heapNum = (int *)calloc(1, sizeOf(int)); // allocates on the heap return 0; } After the function exits, stackNum will be released from memory but heapNum must be freed by the user\nfree(heapNum); Malloc: # Does not clean up, but is faster than calloc, just be careful. Type *type_name = (Type *)malloc(numOfElements, sizeOf(Type)); free(type_name); //same as calloc Dynamic Arrays # Taking the best parts of Arrays ($O(1)$ lookup, consecutive boxes in memory) but without fixed length constraint.\nEssentially, when an array of size $N$ is at capacity (check using a counter variable), create a new array of size $2N$, and copy existing elements over to the new array\nint *infiniteChocolateCopy(int someChocolate[n], int n) { int *moreChocolate = (int *)calloc(2 * n, sizeOf(int)); // allocates size 2n on the heap for(int i = 0; i \u0026lt; n; i++) { //iterates through someChocolate *(moreChcolate + i) = someChocolate[i]; //copies over } return moreChocolate; } Compound Data Types # Storing multiple components of data types (primitive and/or compound), packaged into a single container. Used when storing information too complex for a single data type.\nDefining a CDT struct: # typedef struct student_struct { // creates the struct char name[1024]; int year; double gpa; Markbook *marks; // example of passing a CDT pointer (CDT-ception) // as many as you want here } Student; Initializing a CDT: # Student *sweaty_nerd = (Student *)calloc(1, sizeOf(Student)); // allocate memory strcpy(sweaty_nerd-\u0026gt;name, \u0026#34;TryHard on Piazza\u0026#34;); // for strings sweaty_nerd-\u0026gt;year = 2023; // arrow (-\u0026gt;) operator for pointers sweaty_nerd-\u0026gt;gpa = 2.718; sweaty_nerd-\u0026gt;marks = bad_marks; Memory Model: # CDT\u0026rsquo;s get one locker for all of it\u0026rsquo;s contents (like a Bento Box) Passing a CDT into a function: myCDT randomCrunchyCDTFunction(myCDT crunch) { // makes a copy (not passed-by-ref) ... return crunch //returns a copy } It is typical to pass and return CDTs as a pointer, instead of copying Abstract Data Types # Implementation independent! It is just the idea of a container that stores a collection of data.\nList ADT: # Stores items sequentially in nodes not necessarily consecutive, containing a reference to the next node in the list Not fixed in length Head: 1st node in list Tail: last node in list Typical operations, $O(N)$ worst case: Insert Remove Update Search Queue ADT: FIFO (first-in-first-out) Stack ADT: FILO (first-in-last-out) Linked List: # Best used when data is added and queried in random order.\nBasic dynamic implementation of List ADT\ntypedef struct linked_list_struct { int data // payload (can be any type) struct linked_list_struct *next; // pointer to the next node in the list, NULL if tail node } ListNode; Iterating through a Linked List\nfor(ListNode *n = head; n != NULL; n = n-\u0026gt;next); Insert\nAt head - $O(1)$ complexity:\nListNode *wantToInsert = (ListNode *)calloc(1, sizeOf(ListNode)); wantToInsert-\u0026gt;data = 51 // coincidentally the course midterm average wantToInsert-\u0026gt;next = head; // where head is the head of the LL At tail - $O(N)$ complexity:\n//same initialzation of wantToInsert as 1. ListNode *n = head; if(head != NULL) { // always check if head is not null to avoid errors for(; n-\u0026gt;next != NULL; n = n-\u0026gt;next); // traverses until at the tail n-\u0026gt;next = wantToInsert; // sets the tail as wantToInsert } Somewhere in between\n//same initialzation of wantToInsert as 1. ListNode *n = head; if(head != NULL) { for(; n-\u0026gt;data != insertHereNum; n = n-\u0026gt;next); // traverses list wantToInsert-\u0026gt;next = n-\u0026gt;next; // links wantToInsert to n-\u0026gt;next n-\u0026gt;next = wantToInsert; // links n to wantToInsert } Note: When inserting inside the list, we would use a condition to compare unique identifiers so we know exactly where to insert. For this example we assumed all the numbers were distinct and we knew insertHereNum\u0026rsquo;s value was given and inside the Linked List.\nSearch - $O(N)$\nTraverse from head Check if current node is the desired node using a compare operation/function (see Note) Return A pointer of the desired node, user can modify the node and list A copy of the data type (compound/primitive), user cannot modify the node and list Delete - $O(N)$\nListNode *p1 = head; ListNode *p2 = NULL; // is always 1 node behind p1 if(head != NULL) { for(; p1 != NULL \u0026amp;\u0026amp; p1-\u0026gt;data != wantedNum; p2 = p1, p1 = p1-\u0026gt;next); //traverses through LL, one behind the other until wantedNode is found if(p1-\u0026gt;data == head-\u0026gt;data) p2 = head-\u0026gt;next; // only if the wantedNode is the head else p2-\u0026gt;next = p1-\u0026gt;next; // all other cases, links the node before wantedNode with the node after free(p1); // releases the node from the list } Unit 4: # Computational Complexity: # Measuring the amount of work done by an algorithm as a function of the number of data items the algorithm is working on, and thus predicting how algorithms will preform against each other without having to test on large $N$ values.\nThe Big $O$ Notation # Comparing algorithms in a machine and implementation dependent manner.\nMathematically put, $$ f(x) = O(g(x)) \\iff \\exists c \\in \\mathbb{R}_{\u0026gt;0} \\text{ such that for sufficiently large } x \\text{, } \\newline |f(x)| ≤ c \\text{ } \\cdotp{g(x)}, \\quad x \u0026gt; x_0 $$\n$O(g(x))$ is the smallest function of $N$ that puts an upper bound on $x$\nGiven a set of candidate algorithms, the fastest algorithm will have the slowest growing Big $O$ complexity.\nIn terms of efficiency, $$ O(1) \u0026lt; O(log (N)) \u0026lt; O(N) \u0026lt; O(Nlog(N)) \u0026lt; O(N^2) \u0026lt; O(N^3) \u0026lt; O(2^N) \u0026lt; O(N!) $$\nBinary Search for Arrays # Array must be sorted Complexity of $O(log_2(N))$ Procedure (Pseudocode): int binarySearch(int wantedNum, int Array[]) { int middleNum = Array[floor(length / 2.0)]; // finds middle index (floor if odd length) if(wantedNum == middleNum) { return index(middleNum); // returns index of middleNum in Array } else if (wantedNum \u0026gt; middleNum) { binarySearch(wantedNum, Lower Half of Array); // all values greater than middleNum } else { binarySearch(wantedNum, Upper Half of Array); // all values less than middleNum } } Complexity of Linked Lists versus Arrays # All basic operations on a Linked List have worst case $O(N)$ complexity. Unsorted Array: Search: Index lookup: $O(1)$ Linear search: $O(N)$ Sort: make a reasonable assumption :^) Sorted Array: Search: Index lookup: $O(1)$ Binary Search: $O(log(N))$ The Consequence of Sorting # Bubble Sort:\nTraverse the array and swap adjacent decreasing entries until the array is sorted.\nWorst-Case Complexity: $O(N^2)$\nvoid notSoCrunchyBubbleSort(int array[], int N) { for(int i = 0; i \u0026lt; N; i++) { // N iterations for(int j = 0; j \u0026lt; N - 1; j++) { // N - 1 iterations if(array[j] \u0026lt; array[j-1]) { // compares if the adj. entries are decreasing swap(array[j], array[j+1]); } } } } Quick Sort (qsort):\nChoose a random pivot, split elements into 2 arrays: values less than the pivot, and values greater than or equal to the pivot. Repeat until all sub-arrays have lengths ≤ 1. Reconstruct the now sorted array.\nAverage-Case Complexity: $O(Nlog(N))$ (not so crunchy) Worst-Case Complexity: $O(N^2)$ (first/last entry pivot $\\rightarrow$ insertion sort) Insertion Sort:\nBuild the sorted array in place (no splits), shifting elements as we traverse the array to sort.\nBest-Case Complexity: $O(N)$ Worst-Case Complexity: $O(N^2)$ Procedure: Choose first element to be \u0026ldquo;sorted\u0026rdquo; Look at next element in array, and insert it inside the \u0026ldquo;sorted\u0026rdquo; portion by comparing it to the values in said portion. Repeat 2. until the end of array Trees # A generalization of Linked Lists, linking one node in the Tree to sucessor (children) nodes. Recursive in nature, each sub-tree is also a Tree.\nBinary-Search Trees (BST) # Variation of a Binary Tree (left and right children only) such that the BST property holds for each node and duplicate nodes are not allowed.\nBST Property:\nData in nodes on the left sub-tree are less than or equal to data in the root node Data in nodes on the right sub-tree are greater than data in the root node Basic Implementation\ntypedef struct dollarStoreChocolate_BST_Struct { int data; struct dollarStoreChocolate_BST_Struct *left; // left child pointer struct dollarStoreChocolate_BST_Struct *right; // right child pointer } BST_Node Search - $O(log(N))$:\nif(root == NULL) return NULL; else if(givenData == root-\u0026gt;data) // check root return root; else if(givenData \u0026lt;= root-\u0026gt;data) return search(root-\u0026gt;left, givenData); // search left sub-tree else return search(root-\u0026gt;right, givenData); // search right sub-tree Insert - $O(log(N))$\n// assuming new_node was initialized and correctly allocated to the heap if (root == NULL) // empty tree return new_node; // inserts at root else if (new_node-\u0026gt;data \u0026gt; root-\u0026gt;data) root-\u0026gt;right = insert(root-\u0026gt;right, new_node); // inserts in right sub-tree else root-\u0026gt;left = insert(root-\u0026gt;left, new_node); // inserts in left sub-tree return root; Note: Just like Linked Lists, we must use our own comparison function if the data in the node is a CDT.\nTraversal - $O(N)$\nIn Order - For listing a BST in sorted order: if (root != NULL) { inOrder(root-\u0026gt;left); // traverses left sub-tree printf(\u0026#34;%d\\n\u0026#34;, root-\u0026gt;data); // any operation can be preformed inOrder(root-\u0026gt;right); // traverses right sub-tree } Pre Order - For Copying an entire BST: if (root != NULL) { printf(\u0026#34;%d\\n\u0026#34;, root-\u0026gt;data); // operation preOrder(root-\u0026gt;left); // traverses left sub-tree preOrder(root-\u0026gt;right); // traverses right sub-tree } Post Order - For deleting an entire BST: if (root != NULL) { postOrder(root-\u0026gt;left); // traverses left sub-tree postOrder(root-\u0026gt;right); // traverses right sub-tree printf(\u0026#34;%d\\n\u0026#34;, root-\u0026gt;data); // operation } Delete - $O(log(N))$\nSearching for node to delete is the same implementation as Insert and Search No children: free(root); // if only the final exam was as simple as this One Child: Left only BST_Node *temp = root-\u0026gt;left; // points to left child free(root); return temp; Right only BST_Node *temp = root-\u0026gt;right; // points to right child free(root); return temp; Two Children: BST_Node *temp = find_successor(root-\u0026gt;right); // smallest node in the right subtree copyNode(root, temp); // copies all the data from temp to the root root-\u0026gt;right = delete(temp); // deletes the successor recursively return root; Complexity of Binary Search Trees versus Linked Lists and Arrays # Building the data structure:\nLL: $O(N)$ BST: $O(Nlog(N))$ Array + Merge Sort: $O(Nlog(N))$ Search:\nLL / Unsorted Array: $O(N)$ BST: $O(Log(N))$ Sorted Array: $O(Log(N))$ Space Complexity:\nArray: Fixed size LL: $O(N)$ BST: $O(N)$ Unit 5: # Graphs: # A model to represent items and their relationships between them. Composed of Nodes (Verticies) and Edges, with optional direction and weight.\n$$ G=(V,E) $$\nGraph Representation # Direction:\nUndirected: Two way relationship Directed: One way relationship Neighbourhood:\nThe neighbourhood of Node U is the set of all nodes that are neighbours of Node U.\nNeighbour: 2 Nodes are considered neighbours if they are joined by an edge\nDirected Example: Node U $\\rightarrow$ Node V\nIn-neighbour: U of V\nOut-neightbour: V of U\nIn-neighbourhood: Edges arriving at Node U\nOut-neighbourhood: Edges leaving Node U\nDegree:\nThe size/dimension (number of nodes) in the neigbourhood of the Node.\nIn-degree: Degree of In-neighbourhood Out-degree: Degree of Out-neighbourhood Traversals:\nBreadth First Search (BFS) - By path of neighbours Depth First Search (DFS) - Level by level General Applications of Graphs # Social Networks\nTransportation (Maps services)\nGenomics and Bioinformatics\nComputer Networks and the Internet\nRepresenting Graphs # Adjacency List:\nAn array with one entry per node. Each entry points to a linked list containing the neigbourhood for that node.\nAdjacency Matrix:\nA 2D $N\\text{ x }N$ matrix, $N$ is the number of nodes in the graph. If Node $i$ and Node $j$ share an edge then AdjMat[$i$][$j$] \u0026gt; 0. For Undirected graphs, AdjMat[$i$][$j$] $=$ AdjMat[$j$][$i$]\nOperation Complexity on Graphs # $N = |\\text{Vertices}|$ is the number of nodes in the graph, and $M= |\\text{Edges}|$ is the number of edges in the graph\nOperation Adjacency List Adjacency Matrix Edge Query $O(N)$ $O(1)$ Inserting a Node $O(1)$ $O(N^2)$ Removing a Node $O(M)$ $O(N^2)$ Inserting an edge $O(1)$ $O(1)$ Removing an edge $O(N)$ $O(1)$ Principles of Recursion: # The repeated application of a recursive procedure. Can make some problems super trivial to solve.\nTypes of problems that benefit from recursion: # Sudoku, N Queens (pretty c r u n c h y) BST Operations (duh) Graph Operations (every sub-graph is still a graph) Search and Path Finding (BFS and DFS) Generally any problem that contains a smaller version of itself as a sub-problem.\nThe process of designing a recursive solution # Base Case(s): Specific to the problem itself, multiple can exist The smallest problem with a trivial solution Any solution must always reach the base case Recursive Case: Multiple ways to split the problem at hand, some better than others After each recursive call, the sub-problem must be closer to the base case Always best to visually interpret/draw out the solution Recursive Sort and their complexities # DoofusSort/chewySort/notAProGamerSort:\nFirst Entry goes into one sub-array, the rest into another Recursively sort each sub-array Merge to form the sorted array Complexity:\nWorst Case: $O(N^2)$ (Essentially Insertion Sort) Merge Sort:\nChoose the middle entry as pivot Split into 2 sub-arrays, one less than, one greater than or equal Recursively sort each sub-array Combine to form the sorted array Complexity:\nWorst Case: $O(Nlog(N))$ (Super Duper Crunchy) qsort:\nSee: The Consequences of Sorting\nThe Memory Model # Each time a recursive call has been made, part of the stack is reserved (stack frame) for variables, parameters and return type. Each stack frame will only be cleared until each recursive call is completed.\nStack Overflow (like the webpage): Results when the stack has ran out of empty space Recursive solution must reach the base case in a reasonable number of steps Tail Recursion can prevent this The recursive call would be the last thing the function does before returning Similar to an iterative solution, better for DEEP recursive solutions Unit 6: # Software Design: # Developers are lazy, we need modular solutions to be as useful for others as possible.\nProperties of good software design # Modularity\nDoes one thing, ridonkulously well. Minimizes code replication Simplifies testing and debugging Reusablity\nAny module can be reused by other applications requiring that specific task. Extendibility\nSoftware that is easy to improve and extend its functionality and usability. Maintainablity\nOrganized, explicit and so well-commented that a noob would understand it. Correctness\nDevs may be lazy, but they aren\u0026rsquo;t super lazy. Include detailed documentation and test cases. Efficiency\nGotta go fast, $O(1)$ or bust. Openness\nYou\u0026rsquo;re not a giant tech company, contribute to the open source software community. Privacy and Security\nSecure data exchanges and reliable and safe solutions over personal/enterprise networks. Application Programming Interfaces (APIs) # Interacting with software modules, without having to understand how they work internally.\nUse-case: Specific situation in which components of the API may be used Dependency: The relationship of one module requiring another module in order to function Properties of a good API # Easy to maintain Easy to extend and improve Easy to learn Difficult to use incorrectly Suitable for those who will be using it Expanding an API # Must consider required use-cases for each module Applications that rely on the API must not experience any significant impacts if expanded Method overloading is a good workaround for any CRUNCHY use-cases after the initial rollout If a bad API is expanded/modified, any software relying on said API will most likely stop functioning correctly Limitations of C # Basically C is !(OOP).\nNo Encapsulation and Information hiding No Polymorphism or Inheritance No Method overloading Object Oriented Programming (OOP) # Model for developing software components based on Encapsulation.\nEncapsulation: Wrapping required components together to implment the functionality of a data type. Hides data and functionality from the user using Access Control Modifiers to prevent misuse. Access Control Modifiers: Private: Accessed only by the class that its in Protected: Gives inherited classes access to the parent class\u0026rsquo;s private data Public: Can be accessed by any code outside of the class Method Overloading # Methods with the same name, but with different input parameters, return type and implementation.\nUseful for multiple use-cases in which that particular solution is needed\nExample (Java):\npublic int addNumbers(int a, int b) { return a + b; } // same name but different return type and input parameters public double addNumbers(double a, double b, double c) { return a + b + c; } _\nClasses and Objects # The template/blueprint for building objects and their variables and methods.\nComponents of a class: Private member variables Used inside the class only Constructor Creates new instances/objects Assigns values to the input paramenters for the instance Destructor Called when finished with the class Object Methods Any functions declared in the class that an object can use An example of an object class (Java):\nclass ComputerScienceNerd { // initialize the class // initializes member variables for ComputerScienceNerd object private String name; private double gpa; public ComputerScienceNerd(String name, double gpa) { // object constructor // private variables (this.) get the parameter values this.name = name; this.gpa = gpa; } /* * Object methods, public in nature * Call the method with (ComputerScienceNerdObject).method(); */ String getName() { return name; } double getGPA() { return gpa; } void screech() { System.out.println(\u0026#34;bY ThE WaY, dId yOu kNoW I\u0026#39;m iN CoMpUtEr sCiEnCe?\u0026#34;); } } Polymorphism and Inheritance # Inheritance:\nThe idea of passing methods and definitions to a hierarchy of Children classes.\nA Child class inherits the same \u0026ldquo;traits\u0026rdquo; as the Parent class Example: Parent Class: Instrument Child Class: WindInstrument inherits Instrument Child Class: Flute inherits WindInstrument The \u0026ldquo;is a(n)\u0026rdquo; Test implies possible inheritance \u0026ldquo;Flute is an instrument\u0026rdquo; $\\rightarrow$ Flute can inherit Instrument Polymorphism:\nThe idea that similar classes should be used the same way. Different objects belonging to the same inheritence hierarchy may posses the same functions, but the behaviour is characteristic to the object itself.\nExample (Pseudocode):\nClass Instrument { Instrument() { double duration; double freq; } playNote() { play(duration, freq); // generic implementation set by Parent class } } Class Guitar extends Instrument { // child of Instrument Guitar() { Instrument(); // uses the same constructor as parent class } playNote() { // overrides the playNote() from the parent // implementation here is specific to the Guitar class } } Class Piano extends Instrument { // child of Instrument Piano() { Instrument(); // uses parent constructor } playNote() { // overrides the playNote() from the parent // implementation here is specific to the Piano class } } Obviously, a Guitar and a Piano do not sound the same (unless you\u0026rsquo;re deaf). As a result, playNote() should be modified for each different sounding instrument. The idea/functionality of playNote() must be the same, regardless of which Child is calling it.\nAbstract Classes # A class that only declares methods, and any subclass must provide implmentations for each method.\nA Parent class that lets its children hold all implementations Example: Parent Class: Instrument Methods: playNote(), tunePitch(), smashOnSomeonesHead() No implementations Children Classes: Piano, Guitar Methods: Same as parent but each child has a different implementation Piano: smashOnSomeonesHead() $\\rightarrow$ \u0026quot;Smash Head on Piano\u0026quot; Guitar: smashOnSomeonesHead() $\\rightarrow$ \u0026quot;Smash Guitar on Head\u0026quot; "},{"id":28,"href":"/notes/CSCC01/final-review/","title":"Final Review","section":"CSCC01","content":" CSCC01 - Final Review # Fall 2020 # The fix from StackOverflow isn\u0026rsquo;t working\nSoftware Development - The Agile Mindset # Iterative approach to software development\nValues of the Agile Manifesto: # Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan User Stories # Short, simple descriptions for features as told by the end-user\nCommonly follows the template:\nAs a \u0026lt;User\u0026gt;, I want (to) \u0026lt;Goal\u0026gt;, (so that)/(in order to) \u0026lt;Value\u0026gt; Never more than 2 brief sentences, but usually one\nTemplate should not mention any technical detail/requirement, only end-user related details\nOften written on index cards, but can be referenced on a Sprint/Kanban board\nAll three attributes (User, Goal, Value) must be mentioned in a user story\nTo further add detail to a User Story, mention a clear Acceptance Criteria or if need be, break it down into smaller stories/sub-tasks\nWhat makes a good user story? (INVEST)\nIndependent Negotiable Valuable Estimatable Sized Appropriately Testable Burn Down Chart # Tool for collecting sprint/project data\nhttps://www.projectmanager.com/blog/burndown-chart-what-is-it\nUsed for viewing how much work is left w/r/t how much time is left\nUsually, x-axis is measured in days/weeks, and y-axis is measured in story points\nThe \u0026lsquo;Ideal Work Remaining Line\u0026rsquo; is a straight line connecting the start of the sprint to the end linearly\nThe \u0026lsquo;Actual Work Remaining Line\u0026rsquo; is the actual work that has been done throughout the sprint, and hence deviates above or below from the ideal\nHaving a visual represenation of progressional data keeps the team more informed and on the same page\nDoesn\u0026rsquo;t show whether the team is working on the right things\nDepending on how well of an estimate user stories are story pointed, this chart may not accurately show if a team is on track or not\nCritical Path # The sequence of activities that will take the longest to complete, determining the overall length of the project\nhttps://www.mountaingoatsoftware.com/blog/the-critical-path-on-agile-projects\nCan be answered 2 ways:\nWhat is the CP within an Iteration? What is the CP within a project? CP can be quickly identified when certain user stories lead/build upon the next/previous one\nEasy to spot due to a short iteration length\nMust be considered by the team, but does not have to be very formal\nUseful when you need to consider future dependencies along the line that may affect what is currently being developed\nAgile Frameworks # Scrum\nSet of meetings, tools, and roles that work in concert to help teams structure and manage their work.\nhttps://www.atlassian.com/agile/scrum\nCentered around continuous improvement and adjustment\nStructured to help teams natually respond and adapt to change in a timely manner\nScrum Master dedicated to co-ordinating the scrum and resolving blockers\nSprints:\nShort, time-boxed period of time where the scrum works to complete a set amount of work Common sprint length is 2 weeks, but can range from 1 week to 1 month Scrum Meetings:\nSprint Planning Meeting:\nDetermine the User and Technical Stories that will be worked on during the current sprint Sprint Estimates (Story Pointing) may or may not occur during this meeting Daily Standup:\nBrief meeting involving the whole scrum, highlighting progress and identifying blockers Must be done at the same time every day, preferably at the same location Usually done standing up, but should reference the Scrum Board No \u0026lsquo;code-talk\u0026rsquo; during this meeting Participants must answer the following: What have you done since yesterday? What are you planning on doing today? Any impediments or stumbling blocks? Backlog Refinement:\nStory Point items found in the backlog Common pointing schema is Fibonacci (1, 3, 5, 8, 13,\u0026hellip;) Story Points represent the relative required amount of effort Must refer to Definition of Done (DoD) and collective effort (Dev + QA) Sprint Retrospective:\nMeeting to reflect on the completed sprint Must ask what went well, and what did not Solutions to fix mistakes are proposed for the next sprint Kanban\nReal time communication of capacity and full time transparency of work\nhttps://www.atlassian.com/agile/kanban\nWork items are represented on a central Kanban Board Board should be seen as the \u0026lsquo;single source of truth\u0026rsquo; for the team\u0026rsquo;s work Entire team\u0026rsquo;s responsiblity to ensure work is moving smoothly through the board Each work item is represented by a single Kanban Card and is positioned in swimlanes based on the state of completion Example swimlanes include To Do, In Progress, In Review, Waiting on QA, Done Test Driven Development (TDD)\nDevelop tests before writing the code.\nThe tests become sort of like the \u0026lsquo;acceptance criteria\u0026rsquo; or specification for development\nTraditionally, TDD means:\nWrite failing test cases Write the minimum amount of code to pass the test(s) Repeat with occasional refactoring Agile teams can adopt TDD with different types of testing:\nUnit Testing Integration Testing Product Acceptance Criteria (Customer Requirement) Testing Regression Tesing Extreme Programming (XP)\nIterative Incremental model incorperating TDD\nCustomer\u0026rsquo;s decisions drive the product Development team works directly with Product Owner or Domain Expert Focused on delivering working software rather than documentation Software Design # It\u0026rsquo;s a design choice\nCoupling # How much a class is directly linked to another class\nHigh coupling between classes means that changes to one class may lead to changes in the other coupled classes\nLow coupling is desired\nCohesion # How much the features of a class belong together\nLow cohesion means that methods in a class operate on unrelated tasks. This means the class does jobs that are unrelated\nHigh cohesion means that the methods have strongly-related functionaly.\nDependency Injection # Seperate the responsibility of resolving object dependency from its behaviour\nIt is an Enterprise Design Pattern: used in enterprise applications\nThe Injector module is basically a container, and owns the life cycle of all objects (classes) defined/instantiated under its scope\nMany different ways to implement dependency injection (ex. Dagger2, Angular)\nNeeds to be instructed/configured to signal that a class has certain dependencies and how to resolve them Constructor Injection (Java)\npublic class MarkBooster { private CheatSystem cheater; @Inject // signals injector that MarkBooster has CheatSystem as dependency public MarkBooster(CheatSystem cheater) { this.cheater = cheater; } } Setter Injection (Java)\npublic class MarkBooster { private CheatSystem cheater; // default constructor public MarkBooster() {} @Inject // resolves CheatSystem dependency public void setCheater(CheatSystem cheater) { this.cheater = cheater; } } SOLID principles of design # Single responsiblilty principle: # A class should have one and only one reason to change\nThe responsibility should be encapsulated by the class\nAll services for that class should be aligned with that responsibility\nOpen/closed principle: # Open: Available for Extension\nClosed: Available for use by other modules\nClasses should be open for extension but closed for modification\nAdd new features by extending the class, which may or may not have the same interface(s) as the original class\nLiskov substitution principle: # Subclasses should add to a base class\u0026rsquo;s behavior, not replace it\nIf S is a subtype of T, then objects of type S may be subbed for objects of type T without altering any of the desired properties of the program\nInterface segregation principle: # No client should be forced to depend on methods it does not use\nMany client-specific interfaces are better than one general-purpose interface\nEasier to extend and modify the design\nDependency inversion principle: # High-level code shouldn\u0026rsquo;t depend on low-level code. Both should depend on abstractions. It addition, abstractions shouldn\u0026rsquo;t depend on details\nDevelop high level classes first, then develop lower-level classes\nDetails depend on abstractions, not concretions\n"},{"id":29,"href":"/notes/CSCC43/final-review/","title":"Final Review","section":"CSCC43","content":" CSCC43 - Final Review # Fall 2021 # Thank god the SQL to this course is an elective.\nDatabase Management System (DBMS) # Database: A structured system to persist data over a long period of time, with certain rules imposing it\nDBMS: Software to manage databases\nManages data from multiple databases Enforces rules on the data Types of DMBS # Relational (RDBMS) Examples: PostgreSQL, MySQL, Oracle, SQLite Hierarchical Networked Object-Oriented NoSQL Key Concepts Overview # Data Model # A collection of tools for describing data, data relationships and semantics, and consistency constraints\nTypes of Data Models # Relational Data is stored in tables with rows and columns Entity-Relationship Mainly used for db design Object-based Object-oriented and Object-relational models Semi-structured (XML) Hierarchical Network Database Instance and Schema # Similar to types and variables in programming languages\nLogical Schema: Overall logical structure of the db (variable types)\nPhysical Schema: Overall physical structure of the db\nInstance: Snapshot of the db contents, at a particular point in time (variable value)\nDatabase Engine # A DBMS is partitioned into modules, each with its own responsibility\nFunctional Components:\nStorage Manager Query Processor Transaction Manager Database Users # Who uses databases, and for what?\nNaive Users (Average consumers)\nUses Client-facing app interfaces/GUIs to indirectly communicate with the db Application Programmers\nWrites embedded SQL Sophisticated Users (Analysts + Scientists)\nUses tools to query data from the db Database Admininistrators (DBA)\nHas central control over the db system General functions: Grants authorization to access data Periodically backs up db Monitors disk space and running jobs Schema definition and modification permissions Relational Data Model # 2D Tables logically represent data Relational Algebra is used for manipulating relations (tables) Set-based: Arbitrary row and column ordering Definitions # $R$: Relation/Table Name Attribute: Column heading (defined in schema) Tuple: Row of atomic cells Arity: Number of attributes $m = |\\textrm{schema}(R)|$ Cardinality: Number of tuples $n = |R|$ Domain: Set of allowed values for each attribute null is a member of every domain (can use it anywhere), this may cause problems (not null constraints) Notation # Attributes\ntuple[attr] = value // Alternatively tuple.attr = value Multiple Attributes\ntuple[attr1, attr2] = \u0026lt;value1, value2\u0026gt; // Generally tuple[A1, ..., AN] = \u0026lt;tuple[A1], ..., tuple[AN]\u0026gt; Integrity Constraints # Property that must be satisfied by all database instances\nA db is legal if it satisfies all integrity constraints Tuple + Domain Constraints # Expresses conditions on values of each tuple, independently of others\nTuple Constraint: Multiple Attributes\n// Net, Gross, Deductions are attributes Net = Gross - Deductions Domain Constraint: Single attributes\nSTUDENT_ID NOT NULL AND LENGTH(STUDENT_ID) \u0026gt;= 5 Keys # Set of attributes that uniquely identifies tuples in a relation\nFormal Definition # “A set of attributes $K \\subset R$ is a superkey if $R$ does not contain two distinct tuples $t_1$ and $t_2$ such that $t_1[K] = t_2[K]$”\n$K$ is a minimal superkey for $R$ if $K$ is the smallest key it can be, i.e. $\\nexists$ $K\u0026rsquo;$ | $K\u0026rsquo; \\subset K$ (nothing smaller) Also known as a candidate key Primary Key # Each relation must have a non-null primary key Candidate keys are chosen to be the primary key Relations reference each other using them (foreign keys) Relational Algebra # A procedural language, consisting of a set of operations mapping one relation to another\nQuery Format:\nSELECT tuples FROM a_relation WHERE predicate Operators are compose-able; output is a relation Unary Operators # Select ($\\sigma$) # Removes unwanted rows, preserves arity (same schema as input relation)\nNotation: $\\sigma_p(R)$\n$R$: Relation $p$: Selection predicate, a boolean formula of terms and connectives Example: $\\sigma_{\\textrm{age} \\ge 21}(\\textrm{people})$\nSELECT * FROM people WHERE age \u0026gt;= 21 Project ($\\pi$) # Removes unwanted columns, preserves cardinality (same number of rows as input relation)\nNotation: $\\pi_Y(R)$\nOutputs relation $Y \\subset X$, where $X$ is the set of attributes of $R$ Example: $\\pi_{\\textrm{name, age}}(\\textrm{people})$\nSELECT name, age FROM people Rename ($\\rho$) # Renames attributes or the entire relation\nUseful for when comparing a relation with itself (comparing one person’s age with everyone else)\nNotations\n$\\rho_{S(A, B)}(R)$: Rename the attributes of $R$ to $A, B$, then rename the relation as $S$ $\\rho_S(R)$: Rename the entire relation as $S$ (attributes do not change) $\\rho_{A = X, B = Y}(R)$: Rename attributes $A$ and $B$ only (relation does not change) Binary Operators # Cartesian Product ($\\cross$) # Combines information from any two relations (Cross Product)\nRenames may be required when two relations share common attributes\nNotation: $r \\cross s$\nOutputs the cross product of $r$ and $s$ Natural Join ($\\bowtie$) # Also combines two relations into a single relation, accounts for attribute names\nTuples are joined if the attribute is shared between both relations\nNotation: $T = R \\bowtie S$\n$\\textrm{schema}(T) = \\textrm{schema}(R) \\cup \\textrm{schema}(S)$\n$|T| \\le |R| * |S|$, usually around $\\textrm{max}(|R|, |S|)$\nCommutative, Associative, and no ambiguous N-Arity joins ($A \\bowtie B \\bowtie C$ is unambiguous)\nEquivalent to $\\pi(\\sigma_\\theta(R \\cross S))$\nProjects away attributes Special schema cases:\nIf there is no overlap: $\\bowtie$ = $\\cross$ and $|T| = |R| * |S|$ If there is complete overlap: $\\bowtie$ = $\\cap$ Theta Join ($\\theta$) # Returns the pairwise combo of rows that satisfy $\\theta = p$, where $p$ is some predicate\nNotation: $T = R \\bowtie_\\theta S$\n$|T| \\le |R| * |S|$ Doesn’t have to be an equality predicate, any works\nEquivalent to $\\sigma_\\theta(R \\cross S)$\nNo overlap in schemas Doesn’t project away attributes, doesn’t discard dangling tuples Equijoin # Special case of Theta Join Notation: $T = R \\bowtie_{A = X, B = Y \\dots} S$ Attributes names in $R$ and $S$ can differ Like Natural Join, but for arbitrary attributes Equivalent to $R \\bowtie \\rho(S)$ Outer Join (⟗) # Extends the Inner Join by padding the relation with NULL ($\\bot$) values\nNotation: $T = R ⟗ S$\nPadding Type:\nLEFT (⟕): Pads tuples in the left relation with NULL if there is no matching tuple in the right relation RIGHT (⟖): Reverse of left join (pads right) FULL (⟗): Pads both Additive Operators ($\\cup$, $\\cap$, $-$) # Operates on tuples within the relations, and not on the schema\nPrerequisite for relations $r$ and $s$:\nMust have the same Arity\nAttribute domains/types of both must be compatible\nUnion ($\\cup$)\nCombines two relations with into one\nTuples are not repeated if they are in common, only one tuple will result\nRenaming attributes might be require if attribute names differ on both relations\nNotation: $r \\cup s$\nIntersection ($\\cap$)\nOutputs tuples that are present only in both relations\nNotation: $r \\cap s$\nDifference ($-$)\nFinds tuples that are present in one relation but not in the other\nNotation: $r - s$\nDivision ($/$) # Just think of integer division, but for relations\nPrerequisite for relations $r$ and $s$: The attributes of $s$ are found in $r$\nNotation: $T = r / s$\n$T$ is the largest possible set such that $(s \\cross T) \\subseteq R$ If tuple $x \\in T$ and tuple $y \\in S$, then the tuple $x || y \\in R$ (concatenation) Non-Set Extensions # Bag Semantics # Relations are bags (multi-sets) Set semantics for Bags\nUnion: Unordered concatenation\nIntersection: Minimum count of each value in left bag and right bag\nDifference: Takes the difference between the occurrences in the right bag and the left\nUnion and intersection no longer distribute\nDuplicate Elimination ($\\delta$) # Turns a bag into a set\nNotation: $T = \\delta(R)$\nAggregation # SUM, AVG, MIN, MAX, COUNT\nIncludes the duplicates\nGrouping ($\\gamma$) and Aggregating # Group tuples based on a similar/related key\nGrouping key: subset of attributes to test equality\nNotation: $\\gamma_{A, B, C, f(X), g(Y), h(Z)}(R)$\nGrouping keys: $A, B, C$\nAttributes: $X, Y, Z$\nCommutative aggregate Functions: $f, g, h$\nExtended Projection ($\\pi_L$) # Notation: $\\pi_L(R)$\n$L$ contains any of the following:\nOne attribute of $R$\nRename of attributes expression $x \\rightarrow y$\nRename expression $E \\rightarrow z$\n$E$: expression involving attributes of $R$, constants, arithmetic \u0026amp; string operators $z$: new name of the resulting attribute (return value of $E$) Sorting ($\\tau$) # Sorts tuples in a relation based on its attributes\nNotation: $\\tau_L(R)$:\n$L$: List of tuples, sort is based on the first tuple, fall-back to the next if tie Default is ascending order, $-$ is descending Constraints # Suppose $R$ and $S$ are expressions in Relational Algebra\nNotation:\nEmpty Equals: $R = \\empty$\nEquivalently, $R \\subseteq \\empty$ Subset: $R \\subseteq S$\nEquivalently, $R - S = \\empty$ Example:\nSchema:\n-- Primary Key: certificateID MovieExecutive ( name: string, address: string, certificateID: int, netWorth: int ) -- Primary Key: name -- Foreign Key: presidentCertificateID Studio ( name: string, address: string, presidentCertificateID: int ) Problem:\nTo be a president of a studio, you must have a net worth of at least $10 000 000 Constraint:\n$\\sigma_{\\text{netWorth} \\lt 10,000,000}(\\text{Studio}\\bowtie_{\\text{presidentCertificateID } = \\text{ certificateID}} \\textrm{MovieExecutive}) = \\empty$\nThe relation of selecting all tuples from the theta-joined relation where netWorth is strictly less than 10 Million must be empty Alternatively, $\\pi_{\\textrm{presidentCertificateID}}(\\textrm{Studio}) \\subseteq \\pi_{\\text{certificateID}}(\\sigma_{\\text{netWorth} \\ge 10,000,000}(\\textrm{MovieExecutive}))$\nAll of the presidentCertificateIDs found in Studio, must be found in all of the certificateIDs of all MovieExecutives that have a netWorth of greater than or equal to 10 Million Limitations of Relational Algebra # RA is set based\nExpensive to eliminate duplicates, order output\nNeed a way to apply scalar expressions to values\nValues/Attributes that are not there, can be more important than what is there\nStructured Query Language (SQL) # Also known as, \u0026ldquo;The easiest part of this course\u0026rdquo;\nNon procedural; One query always returns one table Not a Turing machine equivalent language Usually embedded in other languages (Java, JS, Ruby) Data Manipulation Language (DML) # Query Language (creating, reading, updating, deleting data)\nGeneral Query Syntax # SELECT \u0026lt;DISTINCT?\u0026gt; \u0026lt;attribute\u0026gt; AS \u0026lt;renamed_attribute\u0026gt; FROM \u0026lt;tables | subquery\u0026gt; WHERE \u0026lt;predicate on tuples\u0026gt; GROUP BY \u0026lt;attributes\u0026gt; HAVING \u0026lt;predicate on groups\u0026gt; ORDER BY \u0026lt;attributes\u0026gt; \u0026lt;ASC | DESC\u0026gt;; -- Set Operations (\u0026lt;query_1\u0026gt;) \u0026lt;UNION | INTERSECT | EXCEPT\u0026gt; (\u0026lt;query_2\u0026gt;) Example syntax for common queries # Joining two tables SELECT * FROM table_1 \u0026lt;NATURAL | CROSS | INNER | OUTER | LEFT | RIGHT\u0026gt; JOIN table_2 ON table_1.attr1 = table_2.attr2; -- Cartesian Product SELECT * FROM table_1, table_2; Getting the count of an attribute SELECT attr, COUNT(attr) AS attr_count FROM my_table GROUP BY attr; -- GROUP BY must be included Inserting into another table INSERT INTO my_table ( -- Query goes here ); Updating one tuple UPDATE my_table SET \u0026lt;attribute\u0026gt; = \u0026lt;value\u0026gt; WHERE \u0026lt;predicate on tuples\u0026gt;; Deleting DELETE FROM my_table ( -- Query goes here ); Creating virtual views CREATE VIEW my_view ( -- Query goes here ); DROP VIEW my_view; -- Important! Transactions # Collection of one (or more) atomic and serializable operation(s)\nEach SQL statement is a transaction itself\nCan group several statements together into a single transaction\nIsolation Levels # Dirty Read: reads values written by another transaction that hasn’t committed yet\nNon Repeatable Read: reads the same object twice, but finds a different value on the second time even though the transaction didn\u0026rsquo;t change it\nPhantom Read: transaction re-executes a query, finds that the set of rows returned has been changed based on another committed transaction\nGeneral Syntax # SET TRANSACTION ISOLATION LEVEL \u0026lt;isolation_level\u0026gt;; \u0026lt;isolation_level\u0026gt; Dirty Reads Non Repeatable Reads Phantom Reads READ UNCOMMITTED Possible Possible Possible READ COMMITTED Not Possible Possible Possible REPEATED READ Not Possible Not Possible Possible SERIALIZABLE Not Possible Not Possible Not Possible Data Definition Language (DDL) # Notation for defining a database schema\nSchema Syntax # Basically namespaces.\nCREATE SCHEMA University; -- delete schema DROP SCHEMA IF EXISTS University CASCADE; -- do this if you only use one schema SET search_path to University -- namespaced tables CREATE TABLE University.professors; Built-in Types # CHAR(n) -- length = n VARCHAR(n)\t-- length \u0026lt;= n INT\tSMALLINT\t-- Smaller subset of Integer FLOAT\tBOOLEAN\t-- TRUE / FALSE DATE\t-- \u0026#39;YYYY-MM-DD\u0026#39; TIME\t-- \u0026#39;HH:MM:SS\u0026#39; TIMESTAMP\t-- DATE \u0026amp; TIME Table Syntax # CREATE TABLE my_table ( \u0026lt;attribute_1\u0026gt;\t\u0026lt;type\u0026gt;, \u0026lt;attribute_2\u0026gt;\t\u0026lt;type\u0026gt;, -- ... primary key (attr), foreign key (other_table_key) references other_table ); -- delete table DROP TABLE my_table \u0026lt;CASCADE?\u0026gt;; -- cascade only if other tables reference it -- update table ALTER TABLE my_table ADD COLUMN \u0026lt;attribute\u0026gt; \u0026lt;type\u0026gt;; ALTER TABLE my_table DROP COLUMN \u0026lt;attribute\u0026gt;; Database Design Theory # Domain Knowledge also influences theory\nDefinitions and Concepts # Normalization: Eliminating redundancies in a given database schema by breaking down the relation\nFunctional Dependency: If two tuples have the same values for attributes $A_1, \\dots, A_n$ then they must also have the same values for attributes $B_1, \\dots, B_m$\nSyntax: $X \\to Y$ ($X$ functionally depends on $Y$) Multivalued Dependency: If you fix the values for one set of attributes $A$, then the values in other attributes $B$ are independent from the values of all other attributes in the relation\nSyntax: $A \\to\\to B$ Key: Candidate Key ($K$)\nFD Definition: $K \\to R$, where $R$ is the entire relation Also known as the Minimal Super Key One candidate key becomes the Primary Key Prime Attributes: Attributes that form a Candidate Key\nFunctional Dependency Rules # Trivial FDs # Practically meaningless and do not impose anything on the relation\n-- identity x -\u0026gt; x -- same attribute on both sides abc -\u0026gt; a Splitting Rule # Attributes on the RHS are independent of each other\n-- original abc -\u0026gt; def -- refactored (singleton) abc -\u0026gt; d abc -\u0026gt; e abc -\u0026gt; f Combining Rule # Useful to combine the RHS and identify candidate keys\n-- original xy -\u0026gt; ab yz -\u0026gt; cd -- refactored xyz -\u0026gt; abcd Transitive Rule # Can identify hidden FDs, useful for finding cycles\n-- given x -\u0026gt; y y -\u0026gt; z -- implied by rule x -\u0026gt; z Multivalued Dependency Rules # Splitting/combining not allowed Trivial MVDs # Practically meaningless and do not impose anything on the relation\na1 a2 a3 -\u0026gt;-\u0026gt; b1 b2 b3 -- trivial if {b1, b2, ...} subset {a1, a2, ...} Transitive Rule # Similar to FD rule\n-- given x -\u0026gt;-\u0026gt; y y -\u0026gt;-\u0026gt; z -- implied by rule x -\u0026gt;-\u0026gt; z FD Promotion Rule # All FDs are MVDs\n-- given x -\u0026gt; y -- implied by rule x -\u0026gt;-\u0026gt; y Compliment Rule # $X \\to\\to A$ implies $X \\to\\to (R - A)$\n-- given R = (a, b, c) a -\u0026gt;-\u0026gt; b -- implied by rule a -\u0026gt;-\u0026gt; c Suprising Rule # For a given MVD, and any FD whose RHS is a subset of the RHS of the MVD\n-- given R = (a, b, c, d) a -\u0026gt;-\u0026gt; bc d -\u0026gt; c -- c subset of bc -- implied by rule a -\u0026gt; c Closures # Approach to find all trivial and non-trivial FDs for a given relation ($R$) and FD set ($S$).\nConstruction Algorithm # Split all FDs in $S$ into singletons, and find any implied FDs using transitivity\nList out all combinations of attributes in $R$\nExample: If $R = (A, B, C)$ then $X = {A, B, C, AB, AC, BC, ABC}$) For each combination, list out all other attributes you can reach using the given FDs\nExample: If $S = (AB \\to C)$ then ${AB}^+ = {A, B, C}$ Remove all rows that are trivial (LHS = RHS)\nExample Table # $R = (A, B, C, D, E)$ $S = (AB \\to C, A \\to D, D \\to E, AC \\to B)$ $X$ $X_s^+$ $A$ ${A, D, E}$ $AB$ ${A, B, C, D, E}$ $AC$ ${A, B, C, D, E}$ $D$ ${D, E}$ Minimal Basis # The complete opposite of a closure\nDefinition: FD set $S\u0026rsquo; \\subseteq S$ is a minimal basis if $\\forall X \\in S, S\u0026rsquo;$ entails (can reach) $X$ Not necessarily unique for a given FD set ($S$) Construction Algorithm # Split all FDs in $S$ into singletons, and find any implied FDs using transitivity ($S\u0026rsquo;$).\n$\\forall$ Functional Dependency $X \\in S\u0026rsquo;$, manually test if $(S\u0026rsquo; - X)^+ \\equiv S^+$\ni.e. Remove all redundant FDs that are either trivial or implied $\\forall X \\in S\u0026rsquo;$, for attributes $i \\in \\textrm{LHS}(X)$, let $\\textrm{LHS}(X\u0026rsquo;) = \\textrm{LHS}(X\u0026rsquo;) - i$\nManually test if $(S\u0026rsquo; - X + X\u0026rsquo;)^+ \\equiv S^+$ i.e. Remove any redundant attributes in each FD Repeat with $S\u0026rsquo;$ as the new $S$ until minimal\nFD Projection # Used to refactor FDs when splitting a relation\nGiven a relation $R$ with FD set $S$, decompose $R$ into $R_1$ and $R_2$ with their own FD sets $S_1$ and $S_2$\nEach FD sets must only include attributes from its respective relation\nMany possible projections\nProjection Algorithm # For $R_1$:\nSet $S_1 = \\empty$ Compute the closure of each combination of attributes of $R_1$, and add to $S_1$ all non-trivial FDs $X \\to A$ such that the RHS attribute $A \\in X^+$ and $A \\in R_1$ Construct a minimal basis from $S_1$ Repeat for all decomposed relations ($R_2, \\dots$)\nImprovements # Ignore Trivial dependencies\nIgnore Trivial subsets ($\\empty$ or $X$)\nIf $X$ is a set of attributes such that $X^+ = R$ ($X$ is a super key), then ignore any other sets that contain $X$\nChase Test # Used to infer FDs and MVDs\nFor Functional Dependencies # For original relation $R$, decomposed relations $R_1, R_2$, and FD set $S_F$\nConstruct a tableau using the decomposed relations\nApply FDs from $S_F$ onto the rows of the tableau\ni.e. if a row has $a, b_1, c_1, d$ and another row has $a, b_2, c, d_1$ with FD $A \\to B$ Then $b_1 = b_2$ in this case Repeat until you end up with a row without subscripts\ni.e. if $R = (A, B, C, D)$, you want a row on the tableau that looks like $a, b, c, d$ Note: If you just want to see if a particular FD holds, you can also just compute the closure of the LHS and verify, rather than chasing\nFor Multivalued Dependencies # Same relation and FD set, use $S_M$ as the MVD set\nConstruct a tableau\nApply FDs the same way\nFor MVDs in $S_M$, create new rows (if necessary)\ni.e. if $A \\to\\to B$ is in $S_M$ and R = (A, B, C) Then must have rows with $a, b_1, \u0026hellip;$ and $a, b, \u0026hellip;$ ... must be the same in both rows (want $b$ to be independant of $a$) Repeat until you end with a row without subscripts\nNormal Forms # What it means to have a ‘good’ schema\n1st Normal Form (1NF) # No multi valued attributes (no lists, arrays)\nNo duplicate rows\nNot expressible in RA\n2nd Normal Form (2NF) # Must satisfy 1NF\nNo partial dependencies\ni.e. For all non-prime attributes $a$, $\\exist$ FD $X \\to a$ such that $X$ must be a candidate key\n3rd Normal Form (3NF) # Must satisfy 2NF\nNo Transitive dependencies (LHS must be a prime attribute or a super key)\nDecomposition Steps # For the FD set $S$, find the minimal basis $M$ For all FDs $X \\to A$ in $M$, create a new relation $R_i = (X, A)$ If there isn\u0026rsquo;t a relation whose attributes $K = {k_1, \u0026hellip;, k_m}$ form a super key for $R$, create a new relation $R_k = (k_1, \u0026hellip;, k_m)$ Verify each $R_i$ is in 3NF, if not then decompose further Boyce-Codd Normal Form (BCNF) # Must satisfy 3NF\nNo non-trivial FDs\ni.e. if $X \\to Y$ is a non-trivial FD, then $X$ must be a super key\nDecomposition Steps # For all FDs $X \\to A$ where $R = A \\cup B$, set $R_1 = (X, A)$ and $R_2 = (X, B)$ Project the remaining FDs onto $R_1$ and $R_2$ Verify $R_1$ and $R_2$ are in BCNF, if not then decompose further 4th Normal Form (4NF) # Must satisfy BCNF\nNo non-trivial MVDs\ni.e if $X \\to\\to Y$ is a non-trivial MVD, then $X$ must be a super key\nDecomposition Steps # For all 4NF violations (non-trivial MVDs) $X \\to A$, create two new relations $R_1 = (X, A)$ and $R_2 = (X, R - A)$ Find (project) all FDs and MVDs from $R$ onto $R_1$ and $R_2$ Verify $R_1$ and $R_2$ are in 4NF, if not then decompose further eXtensible Markup Language (XML) # Basically if HTML and JSON had a child\nData-only format: No implied representation\nHeirarchical, Tree like data structure\nOrdering is implied\nFlexible schema\nSyntax similar but stricter than HTML\nTags must always be closed Tag and Attribute names have no meaning semantically Example Syntax (Well-Formed XML) # \u0026lt;? xml version = \u0026#34;1.0\u0026#34; encoding = \u0026#34;utf-8\u0026#34; standalone = \u0026#34;yes\u0026#34; ?\u0026gt; \u0026lt;StarMovieData\u0026gt; \u0026lt;Star starID = \u0026#34;cf\u0026#34; starredIn = \u0026#34;sw\u0026#34;\u0026gt; \u0026lt;Name\u0026gt;Carrie Fisher\u0026lt;/Name\u0026gt; \u0026lt;Address\u0026gt; \u0026lt;Street\u0026gt;123 Maple St.\u0026lt;/Street\u0026gt; \u0026lt;City\u0026gt;Hollywood\u0026lt;/City\u0026gt; \u0026lt;/Address\u0026gt; \u0026lt;Address\u0026gt; \u0026lt;Street\u0026gt;5 Locust Ln.\u0026lt;/Street\u0026gt; \u0026lt;City\u0026gt;Malibu\u0026lt;/City\u0026gt; \u0026lt;/Address\u0026gt; \u0026lt;/Star\u0026gt; \u0026lt;Star starID = \u0026#34;mh\u0026#34; starredIn = \u0026#34;sw\u0026#34;\u0026gt; \u0026lt;Name\u0026gt;Mark Hamill\u0026lt;/Name\u0026gt; \u0026lt;Address\u0026gt; \u0026lt;Street\u0026gt;456 Oak Rd.\u0026lt;/Street\u0026gt; \u0026lt;City\u0026gt;Brentwood\u0026lt;/City\u0026gt; \u0026lt;/Address\u0026gt; \u0026lt;/Star\u0026gt; \u0026lt;Movie movieID = \u0026#34;sw\u0026#34; starsOf = \u0026#34;cf\u0026#34;, \u0026#34;mh\u0026#34;\u0026gt; \u0026lt;Title\u0026gt;Star Wars\u0026lt;/Title\u0026gt; \u0026lt;Year\u0026gt;1977\u0026lt;/Year\u0026gt; \u0026lt;/Movie\u0026gt; \u0026lt;/StarMovieData\u0026gt; Root Element: StarMovieData\nThere must only be one Root for it to be Well-Formed Sub Elements: Star, Name, Address, ...\nAttributes: starID, movieID, starredIn, starsOf, ...\n"},{"id":30,"href":"/notes/CSCC69/final-review/","title":"Final Review","section":"CSCC69","content":" CSCC69 - Final Review # Winter 2023 # We both know I shouldn\u0026rsquo;t be writing a review for this course.\nThreads and Processes # Concurrency # Running multiple processes at the same time\nAble to run more processes than the number of cores Serial execution results in more CPU idle time waiting for I/O User threads are 1:1 mapped to kernel threads (struct thread), can also be many:1 The OS also has its own kernel threads Interrupts # CPU stops running current process, saves its state, and runs interrupt handler (threads/interrupt.c) External: Programmable Interrupt Controller (PIC) handles hardware interrupts Internal: Timer interrupts, Syscalls, Faults (page, segmentation, division by zero) Synchronization # Incrementing and Decrementing are not atomic, may cause race conditions Critical Section # Code that accesses shared data and must be executed atomically Requirement Definition Mutual Exclusion Only one thread can be in critical section at a time Progress A thread in the critical section will eventually exit Bounded Waiting If a thread is waiting to enter critical section, eventually it will Performance No thread should wait too long to enter critical section Disable Interrupts: Technically works, but can cause deadlock (thread hangs forever)\nSemaphores: Accessed by multiple threads\nContains a counter (\u0026gt; 0) and a list of waiting threads Before entering critical section, down the semaphore After, up the semaphore, waking up another thread Locks: Acquired and released by a single thread (binary semaphore)\nCondition Variables: Waits for a condition to be true\ncond_init (\u0026amp;cond); cond_wait (\u0026amp;cond, \u0026amp;lock); // thread calls this to wait for condition cond_signal (\u0026amp;cond); // wakes up one waiter cond_broadcast (\u0026amp;cond); // wakes all threads Deadlock # No process can make progress because each is waiting for an event that only another process can cause Avoided by acquiring resources in a fixed order (no circular wait) Note: Any three conditions can (likely) result in deadlock, doesn\u0026rsquo;t have to be all four Condition Definition Mutual Exclusion One process at a time can use the resource Hold and Wait A process is holding at least one resource and waiting for additional resources No Preemption A resource can be released only voluntarily by the process holding it Circular Wait P1 waiting on P2 waiting on \u0026hellip; PN waiting on P1 Scheduling # Thread Lifecycle # Scheduling Problem: Given a set of processes, decide which process to run next, and for how long Starvation: Thread is prevented from making progress because some other thread has the resources it needs i.e. a higher priority thread is preventing the lower priority thread from running Scheduling Metrics # Metric Definition Throughput Number of processes that complete per unit time (maximize) Turnaround time_finished $-$ time_started (minimize) Response time_responded $-$ time_requested (minimize) Scheduling Algorithms # Kind Definition Preemptive OS can switch to another thread at any time Non-Preemptive OS can only switch to another thread after the running thread terminates First-Come, First-Served (FCFS): Can cause head-of-line blocking (long processes block short processes)\nShortest Job First (SJF): Must know processing time, can cause starvation (short processes starve long processes)\nSRTF: Can consider remaining time instead of total time (preemptive), still leads to starvation Round Robin (RR): Preempt current thread after time quantum (1-100ms), and continue in FIFO order\nDoesn\u0026rsquo;t consider a thread\u0026rsquo;s priority, can cause starvation Multilevel Feedback Queue Scheduling (MLFQS): Preemptive, priority-based scheduling\nAssigned high priority initially, and priority is decreased after each time quantum A Lower priority runs only when the higher priority queue is empty If same priority, round robin is used After a job uses up its time quantum, it moves to the lower priority queue After some time, all jobs are moved back to higher priority Priority Donation # A thread can donate to a lower priority thread Eg. Thread H waiting for a lock held by a thread L H donates to L, allowing L to schedule and release the lock L returns the donation back to H to continue running NESTED DONATION: \u0026#34;H is waiting on A held by M and M is waiting on B held by L\u0026#34; |-------------------| |-------------------| |--------------------| | thread H | |-------------| | thread M | |-------------| | thread L | | priority: HIGH | | lock A | | priority: MED | | lock B | | priority: LOW | | waiting_on: A |------\u0026gt;| holder: M |--------\u0026gt;| waiting_on: B |------\u0026gt;| holder: L |--------\u0026gt;| waiting_on: NULL | |___________________| |_____________| |___________________| |_____________| |____________________| Virtual Memory # Abstraction of physical memory Each process has its own virtual address space OS maps virtual addresses to physical addresses without the process knowing other processes\u0026rsquo; addresses Fragmentation: Inability to use memory over time, avoiding this is impossible Internal: Fixed size pieces, internal waste of space External: Free space is not contiguous, can\u0026rsquo;t allocate a large block (many small holes) Factors required for Fragmentation: Different lifetimes: no fragmentation if all processes have same lifetime Different sizes: no fragmentation if all processes have same size Goal Definition Protection Each process has its own virtual address space Efficiency Reduce memory usage and improve performance Convenience Allow processes to use memory without worrying about physical memory Extensibility Allow processes to use more memory than physical memory Attempts to Solve Virtual Memory # Base and Bound Registers\nMMU translates (and verifies) address at runtime physical_address = virtual_address - base If base \u0026lt;= address \u0026lt;= bound, then address is valid, else trap to OS Causes internal fragmentation, growing process is expensive Segmentation\nDivide process into segments, each with its own base and bound Address is built from segmentation table Causes external fragmentation Simple Paging\nDivide virtual memory into fixed-size pages, physical memory into paged-sized frames Each page has a page table entry (PTE) that maps virtual page to physical page Eliminates external fragmentation, internal fragmentation is small Each memory lookup requires two memory accesses (page table and PTE) Multi-Level Paging Page directory contains one entry per page table (pagedir) Lookup process: (happy path) Fetch PTE from PDE Fetch page frame from PTE Compute physical address from page frame and offset If any of the above steps fail, a page fault occurs (userprog/exception.c:page_fault()) Allows for page growth, but now requires 3 memory accesses (time-space trade-off) Translation Lookaside Buffer (TLB): Cache of recently used PTEs, reduces memory accesses to 2 High hit rate, maintained by the MMU (hardware is always faster than software) Page Replacement # Swap: Move a page from physical memory to disk When a page fault occurs, the OS must choose a page to evict from physical memory into swap Page replacement algorithms strive to minimize page fault rate Belady\u0026rsquo;s Anomaly: Having more physical memory does not automatically mean fewer page faults Page Replacement Algorithms # Basic algorithms: FIFO, LRU, LFU, MFU Combinations also exist Clock Algorithm: Approximation of LRU list_init (frame_list); // circular clock_hand = frame_list.head; // some page frame for (each page access request) { if (page in memory) // hit { page.accessed = 1; } else // miss (eviction) { while (clock_hand.accessed == 1) { clock_hand.accessed = 0; clock_hand = clock_hand.next; } swap (clock_hand, page); clock_hand = clock_hand.next; } } Second Chance: Similar to clock algorithm, head of list after is ready to be evicted void second_chance (void) { struct thread *cur = thread_current (); struct frame_table_entry *frame = get_head_frame (); struct sup_page_entry *spe = frame-\u0026gt;spe; while (pagedir_is_accessed (frame-\u0026gt;owner-\u0026gt;pagedir, spe-\u0026gt;uaddr)) { pagedir_set_accessed (frame-\u0026gt;owner-\u0026gt;pagedir, spe-\u0026gt;uaddr, false); // \u0026#39;rotate\u0026#39; frame to the back of the list list_remove (\u0026amp;frame-\u0026gt;elem); list_push_back (\u0026amp;frame_table, \u0026amp;frame-\u0026gt;elem); frame = get_head_frame (); spe = frame-\u0026gt;spe; } } Memory Allocation # Static: Memory is allocated at compile time (fixed size, stack) int a[100]; // simple, but restricted Dynamic: Memory is allocated at runtime (variable size, heap) char *a = (char *) malloc (100); // contiguous block of memory free (a); // creates fragmentation (need to coalesce) Allocator Data Structures # Bitmap: Array of bits, each bit represents a page\n0 means free, 1 means allocated Allocation is slow, requires linear scan to find sequence of zeros No need to coalesce Free List: List of free pages\nCoalescing is required to prevent fragmentation, merge adjacent blocks Placement Algorithms # First Fit: Find first free block that is large enough\nLinear scan of free list sorted LIFO/FIFO/address, pick first one that is large enough Simple, (often) fastest and most efficient May cause fragmentation near start of memory that must be searched repeatedly Best Fit: Find smallest free block that is large enough\nMinimize fragmentation by allocating space from block that leaves smallest fragment Requires linear scan of free list Worst Fit: Find largest free block that is large enough (opposite of best fit) Buddy Allocation: Round up allocations to power of 2 to make management faster\nFast search and merge (alloc and free) Avoids iterating through free list Avoids external fragmentation Physical pages are kept contiguous Storage Devices # Purpose # It is the OS\u0026rsquo;s responsibility to abstract storage details\nHDD only knows about platters and sectors, SSD only knows about pages and blocks Neither know about files, directories, or processes, but user programs do Memory/Storage Hierarchy:\nBalance between cost, performance, and capacity CPU registers and cache are fastest, but small and expensive Hard disk is slow, but large and cheap Exploit locality of reference to minimize cost Persistence:\n\u0026lsquo;Permanent\u0026rsquo; storage of data Organization, consistency, and management issues are important Hard Disk Drives # Slow for random access\nHDD workloads favor high locality and large request sizes\nShould design FS to generate workloads that have this Disk service time components:\nSeeking (expensive) Rotational Latency (spinning) Data transfer (reading/writing) Disks expose storage as a linear array of blocks\nBlocks are 512 bytes Can read/write whole blocks (nothing partial) Actual location of block is unknown to the OS Preventing Failures\nSolid State Drives # Based on NAND flash cells, and have no moving parts\nSLC: Single Level Cell (1 bit per cell) QLC: Quad Level Cell (4 bits per cell) Faster, reliable, and more power efficient than HDD\nBut can only be written/erased to a limited number of times Cells are organized into pages\nPages are organized into Erase Blocks (not the same as disk blocks)\nWrites are only possible to erased pages Writes are 10x slower than reads, Erases are 10x slower than writes\nErase Blocks are the smallest unit that can be erased\nCauses fragmentation, requires garbage collection, usually done in the background GC copies over an entire block to a new block, then erases the old (fragmented) block SSDs overprovision to account for this (240GB SSD has 256GB of storage) GC and Wear levelling (solution to fixed number of writes) case Write Amplification:\nThe number of writes to the SSD is greater than the number of writes to the file system File Systems # Goals # Abstraction: Hide details of storage devices from user programs (files) Developed using a generic block layer (does not matter if HDD or SSD) Organization: Organize files into directories (logically) Sharing: Allow processes, users, and machines to share files Security: Protect files from unauthorized access Disk Layout # inode: Data structure to represent a file (struct inode) Metadata: File size, permissions, owner, etc. Tracks which blocks (on disk) contains the data stored in the file (struct inode_disk) Layout Strategies # Contiguous Allocation: Allocate blocks in a contiguous sequence\nSimple, only need to keep track of offset and length of file Causes external fragmentation, gaps when a file is deleted Linked Allocation: Allocate blocks in a linked list\nEach block points to the next block in the file Doesn\u0026rsquo;t have to be contiguous, need to keep track of first and last block pointer Good for sequential access, bad for all others Indexed Allocation: Multi-level indirect blocks\n\u0026lsquo;Index\u0026rsquo; block (indirect, double indirect, triple indirect) points to other blocks Need to store the index blocks on disk as well Allows for very large and extendable files Freemap: Bitmap of free blocks, used to allocate new blocks Only need to allocate bit by bit, not contiguously UNIX File System # Simple, easy to use, but terrible performance due to lack of locality\nAccess Paths (UNIX) # Reading a file from disk # open (\u0026quot;/foo/bar\u0026quot;) \u0026ldquo;Find the inode for the file (using the path)\u0026rdquo;\nStart at /, find the inode for the directory Read block that contains the inode Traverse the data in the block to find the inode for foo Repeat for bar Allocate a file descriptor for the file, and return, checking file permissions read () \u0026ldquo;Actually read the file, given the inode\u0026rdquo;\nRead in the first block, use inode to find block location If the offset is past EOF, do nothing Update inode accessed time Update file offset When closing the file, deallocate the file descriptor only\nWriting a file to disk # Same open () as above\nwrite () \u0026ldquo;Actually write the file, given the inode\u0026rdquo;\nIf the offset is past EOF, need to extend the file May need to allocate new blocks, update inode, and freemap Read the inode from disk Write the data to existing/new blocks Depending on indexing strategy, may need to write new indirect blocks Update inode with new length, and write serialized inode to disk Placement Problems in UNIX FS # Generates long seeks\nData blocks allocated randomly in aging file system\nInitial blocks are allocated contiguously, but later blocks are allocated randomly (fragmentation from deletion) Inodes allocated far from blocks\nAccess paths operations requires going back and forth between inode and data blocks File Interfaces # File Descriptors: Managed by the OS\nSTDIN is 0, STDOUT is 1 (reserved) All other open files are identified by a unique integer System calls use the file descriptor to identify the file Can also be used for pipes, and sockets File Pointers: Managed by the C library\nFILE * is a pointer to a struct FILE (defined in stdio.h) Contains the file descriptor, and a buffer for reading/writing Use fopen (), fclose (), fread (), fwrite (), \u0026hellip; to manipulate Used for regular files Questions # Why do we need to open files before reading/writing them?\nNeed to load the file into memory, and allocate a file descriptor, in order to manipulate it open () calls filesys_open (), which calls inode_open (), which loads the serialized inode into memory Files occupy multiple blocks, need to load the inode to find the blocks We don\u0026rsquo;t need to load all the blocks at once, just where they are located To check permissions of the file, if the user is allowed to read/write to the file Why do we need to close files when done with them?\nResource management: FDs are a limited resource, need to free them up File operations may be under a lock, so threads/processes may be blocked Security vulnerability, if the file is still open, it can be manipulated by other processes Write operations may be written to a buffer, closing the file flushes the buffer Why did we not have to open STDOUT before using printf ()? BSD Fast File System (FFS) # Improves performance by exploiting locality\nClustering in FFS # Put sequential blocks in adjacent sectors Place inode near file data in the same cylinder Try to keep all inodes in a directory in the same cylinder group (allocation group) Solutions in FFS # The Large File Exception\nProblem: Putting all blocks for a large file in the same cylinder group hurts locality Use a multi-level index block to store the file\u0026rsquo;s blocks, store blocks in different cylinder groups Indirect blocks: 1 indirect pointer to a block of direct pointers Double indirect blocks: 1 indirect pointer to a block of indirect pointers Problem: Small block size in UNIX (1K)\nLow bandwidth and small max file size Use a larger block (4k) Added Symbolic Links\nProblem: Media failures\nDuplicate master block (superblock) Problem: Device Obliviousness\nUse a generic block layer, so that it doesn\u0026rsquo;t matter if HDD or SSD Modern systems hide device details from the file system Links # Hard Links # Several dir entires point to the same inode UNIX counts number of hard links to an inode inode is free\u0026rsquo;d only when all links are removed (rm) If the original file is deleted/moved, the link is not broken Symbolic (Soft) Links # Just a special file (symlink) with a path to another file Type of symlink is stored in the inode If the original file is deleted/moved, the link is broken, but the file still exists Buffer Cache # Read Ahead # FS preloads the next block into the buffer cache, predicting that it will be needed soon Great for sequential accessed files, unless blocks are scattered across the disk FS tries to prevent this during allocation Protecting Files # Protection System: Access control mechanism for files\nOwner Group (UNIX) Permissions Read Write Execute Access Control List (ACL): List of users and permissions for a file\nFor each file, a list of users and their permissions For each user, a list of files and their permissions Crash Consistency # File system data structures must persist, retain data on disk even if the system shuts down Either unexpectedly (crash, power loss) or planned (graceful shutdown) Consider appending to an already open file, this is not atomic and a crash can happen at any of these steps: Write to the inode Write to the freemap (if allocating a new block) Write to the data block Crash Scenarios # Only one of the steps succeeds\nOnly the inode is updated (File Inconsistency)\nInode points to some data block, thinking that the file has data there But it doesn\u0026rsquo;t, so that data block contains garbage Only the freemap is updated (Space leak)\nBitmap claims that the block is allocated, but it is not Wasting an entire block that could actually be used Only the data block is updated\nNot a problem, since the inode is not updated OS will not know that the data block is allocated, so it will be overwritten later Only 2 (out of the 3) steps succeed\nInode and freemap are updated, but data block is not\nThe block that the inode claims has data in, contains garbage FS metadata is consistent Inode and data block are updated, but freemap is not\nBlock is inconsistent with the freemap, thinking that it is free Freemap will (eventually) overwrite the data block, so the data will be lost Freemap and data block are updated, but inode is not\nSimilar to just the data block being updated No clue which file the data block is allocated for Would not get overwritten, since the freemap knows its allocated Inconsistent with the inode, so really just wasting space Journaling # A solution to crash consistency\nAllows the file system to recover from a crash, by keeping a log of all file system operations\nBefore performing an operation, write it to the log\nIf a crash happens, the file system can replay the log to recover\nSmall section of disk is partitioned just for the journal\nJournal is a circular buffer, so it will eventually overwrite itself (due to finite partition size) Writing to the log: # TxB: Transaction Begin marker Contains transaction identifier (TID) Physical Loggings: Write the (exact block content) data to the log TxE: Transaction End marker Contains same TID Checkpoint # Once journal is safely updated, ready to update the actual FS Issue the three writes to the FS Write the inode Write the freemap Write the data block Recovery during checkpoint crash (successful journal log) # Scan the log for completed transactions\nFind the last TxB and TxE All operations between these two markers are part of the same transaction Just replay the transaction, since the data was physically logged in the journal If TxE is missing, then the transaction is incomplete The data was not written to the log, so the update is lost The FS is still consistent, since the checkpoint was not reached Recovery during Journal write crash # Internally, the disk may write the journal data in any order (abstraction of sectors to linear blocks) TxB and TxE might be written before physical loggings, If disk lost power before writing the data blocks, then the transaction is incomplete but may appear complete Split transaction into two (atomic) transactions: TxB + Phys, and TxE\nIf disk loses power during first transaction, no big deal, as if nothing was logged If lost during the second transaction, TxE never gets written, so the entire transaction is lost/skipped anyway One single transaction, but include a Journal Checksum (commit)\nTxB, Phys, Checksum, then TxE Checksum is computed from all the data in the transaction (including TxB and TxE) When replayed, the entire transaction is valid if the checksum is correct (recompute and compare) Metadata Journaling # Log only the metadata (inode + bitmap) changes Data blocks are written to the filesystem directly, not the journal Write the data block to the FS Journal TxB and Metadata Journal Checksum commit Journal TxE Checkpoint metadata (inode, bitmap) FS is consistent if it crashes before this step Free: Update the journal superblock (this journal log can be overwritten) Credits: Professor Bianca Schroeder, University of Toronto\n"}]